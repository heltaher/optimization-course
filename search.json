[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "What is Optimization?",
    "section": "",
    "text": "Optimization is finding the right balance Optimum between what you want Objectives and what you can Constraints\n\n\n\n\n\nFinding the balance",
    "crumbs": [
      "üè† Home"
    ]
  },
  {
    "objectID": "index.html#traveling-salesperson-problem",
    "href": "index.html#traveling-salesperson-problem",
    "title": "What is Optimization?",
    "section": "Traveling Salesperson Problem",
    "text": "Traveling Salesperson Problem\n\n\n\nFinding the shortest possible route that visits a set of given locations exactly once and returns to the starting point.",
    "crumbs": [
      "üè† Home"
    ]
  },
  {
    "objectID": "index.html#knapsack-problem",
    "href": "index.html#knapsack-problem",
    "title": "What is Optimization?",
    "section": "Knapsack Problem",
    "text": "Knapsack Problem\n\n\n\nSelection of items to maximize value under a capacity constraint.",
    "crumbs": [
      "üè† Home"
    ]
  },
  {
    "objectID": "index.html#profit-maximization",
    "href": "index.html#profit-maximization",
    "title": "What is Optimization?",
    "section": "Profit Maximization",
    "text": "Profit Maximization\n\n\n\nAchieving the highest possible profit.",
    "crumbs": [
      "üè† Home"
    ]
  },
  {
    "objectID": "index.html#improving-vehicle-ride-quality",
    "href": "index.html#improving-vehicle-ride-quality",
    "title": "What is Optimization?",
    "section": "Improving Vehicle Ride Quality",
    "text": "Improving Vehicle Ride Quality\n\n\n\nOptimization can be used to maximize the ride quality of a vehicle, leading to more comfortable transportation.",
    "crumbs": [
      "üè† Home"
    ]
  },
  {
    "objectID": "index.html#optimization-of-aerospace-structures",
    "href": "index.html#optimization-of-aerospace-structures",
    "title": "What is Optimization?",
    "section": "Optimization of aerospace structures",
    "text": "Optimization of aerospace structures\n\n\n\nMinimizing weight and improving performance.",
    "crumbs": [
      "üè† Home"
    ]
  },
  {
    "objectID": "index.html#optimization-in-nature",
    "href": "index.html#optimization-in-nature",
    "title": "What is Optimization?",
    "section": "Optimization in nature",
    "text": "Optimization in nature\n\n\n\nBones are optimized for strength and lightweight\n\n\n\nLink to other examples",
    "crumbs": [
      "üè† Home"
    ]
  },
  {
    "objectID": "documents/10-Study_Guide.html",
    "href": "documents/10-Study_Guide.html",
    "title": "Design Optimization: Study Guide",
    "section": "",
    "text": "This study guide summarizes the key concepts and methods covered in the ‚ÄúDesign Optimization‚Äù lecture series, focusing on mechanical engineering systems. It is designed to reinforce your understanding of the material and serve as a quick reference for exam preparation.\n\n\nDesign optimization presents an organized approach to optimizing the design of engineering systems, illustrating basic concepts and procedures with examples to show their applicability to engineering design problems. The course material forms the basis for a first course on optimum design.\n\n\n\n\nCore Concepts: * Definition of Design Optimization: A systematic and organized approach to improving the design process by framing the design of a system as an optimization problem. It aims to maximize or minimize a specific performance measure while satisfying all other design requirements and constraints. This approach differs from the historical process where improvements were often considered only after substantial investments were recouped, leading to adequate rather than optimal systems. * Importance of Design Optimization: Critical for creating efficient, cost-effective, and competitive engineering systems by minimizing costs (e.g., material, manufacturing, energy) and maximizing desirable outcomes (e.g., performance, reliability, profit). * Overall Process of Designing Systems: An iterative process beginning with need identification, followed by defining specifications, preliminary design, detailed design (accelerated by optimization methods), and ultimately fabrication and use. * Engineering Design vs.¬†Engineering Analysis: * Engineering Analysis: Determining the response of a given system to a given input. * Engineering Design: Determining the parameters of a system to achieve desired performance under given inputs. * Conventional Design vs.¬†Optimum Design Process: Both are iterative, but conventional design aims for an ‚Äúacceptable‚Äù solution based on judgment, while optimum design systematically refines the design towards the ‚Äúbest‚Äù possible solution by formalizing the objective and constraints. * Optimum Design vs.¬†Optimal Control Problems: * Optimum Design: Focuses on finding the best physical parameters or configuration of a system. * Optimal Control: Determines the best way to operate or control an existing system over time. For example, a cruise control mechanism is an optimal control system. * Basic Terminology and Notation: Familiarity with linear algebra (vectors, matrices) and basic calculus (functions, derivatives) is essential. Key terms include: * Design Variables (\\(\\mathbf{x}\\)): Parameters adjusted by the designer. * Objective Function (\\(f(\\mathbf{x})\\)): Single performance measure to be optimized (minimized or maximized). * Constraints: Limitations or restrictions (e.g., material failure, demand, resources). * Equality Constraints (\\(h_k(\\mathbf{x}) = 0\\)) * Inequality Constraints (\\(g_j(\\mathbf{x}) \\le 0\\)) * Side Constraints: Bounds on individual design variables. * Feasible Set (S): Collection of all design points satisfying all constraints. * Optimum Solution (\\(\\mathbf{x}^*\\)): Design yielding the best objective value within the feasible set. * Active/Inactive Constraints: Constraints satisfied as an equality are active; otherwise, they are inactive. * Gradient (\\(\\nabla f(\\mathbf{x})\\)): Vector of partial derivatives, pointing in the direction of steepest increase. * Hessian Matrix (\\(\\mathbf{H}(\\mathbf{x})\\)): Matrix of second-order partial derivatives.\n\n\n\n\nCore Concepts: * Importance of Formulation: The proper definition and formulation of a problem can take over 50% of the total effort to solve it, as the optimum solution is only as good as its formulation. * Iterative Nature: Developing a proper formulation is often an iterative process in itself, requiring several revisions. * Five-Step Procedure for Problem Formulation: 1. Project/Problem Description: A clear descriptive statement outlining overall objectives and requirements. 2. Data and Information Collection: Gathering all necessary data (material properties, loads) and analysis expressions. 3. Definition of Design Variables (\\(\\mathbf{x}\\)): Identifying parameters that describe the system and can be freely assigned values by the designer. They should be as independent as possible and precisely defined, including units. 4. Optimization Criterion (Objective Function \\(f(\\mathbf{x})\\)): A single scalar measure to be minimized (e.g., mass, cost, energy) or maximized (e.g., efficiency, strength, profit). Multiple objectives lead to multiobjective problems. 5. Formulation of Constraints: Expressing all limitations (performance, geometric, resource, side constraints) mathematically as equalities (\\(h_k(\\mathbf{x}) = 0\\)) or inequalities (\\(g_j(\\mathbf{x}) \\le 0\\)). Side constraints are bounds on variables (e.g., \\(x_{iL} \\le x_i \\le x_{iU}\\)).\nExamples: * Minimum-Weight Tubular Column Design: Minimizing mass of a column subject to stress, buckling, and manufacturing constraints, with mean radius (\\(R\\)) and wall thickness (\\(t\\)) as design variables. * Maximize Volume of a Beer Mug: Maximizing volume subject to height, radius, and surface area limits, with radius (\\(R\\)) and height (\\(H\\)) as design variables.\n\n\n\n\nCore Concepts: * Applicability: Best suited for problems with two design variables, providing visual intuition into optimization concepts. * Graphical Solution Process: 1. Define Design Space: Establish a 2D coordinate system for the two design variables (\\(x_1, x_2\\)). 2. Plot Constraints and Identify Feasible Region: * Inequality constraints (\\(g_j(\\mathbf{x}) \\le 0\\)) define half-planes; equality constraints (\\(h_k(\\mathbf{x}) = 0\\)) define lines or curves. * The feasible region is the area where all constraints are simultaneously satisfied. 3. Plot Objective Function Contours: Draw lines/curves of constant objective function values (\\(f(\\mathbf{x}) = C\\)). These are called iso-cost or iso-profit lines. 4. Identify the Optimum Solution: ‚ÄúSlide‚Äù the objective function contour in the direction of improvement (decreasing for minimization, increasing for maximization) until it just touches the feasible region. The point(s) of last contact are the optimum. * Properties of Optimal Solutions in 2D: * The optimum is often at a vertex (corner point) of the feasible region. * If the objective function contour is parallel to an active constraint, there might be multiple optimal solutions along that edge. * Active Constraints: Constraints that are satisfied as equalities at the optimum point.\nExamples: * Maximize Volume of a Beer Mug: Graphically solved by plotting constraints on radius and height and sweeping volume contours. * Minimum-Weight Tubular Column Design: Graphically solved using given data to find optimal mean radius and thickness. * Profit Maximization Problem: A classic example to demonstrate plotting constraints and objective function contours to find maximum profit. * Use of Software: MATLAB and Mathematica can be used to plot functions and visualize graphical solutions.\n\n\n\n\nCore Concepts: * Definition of Linear Programming (LP): An optimization technique for problems where the objective function and all constraints are linear functions of the design variables. * General Mathematical Model for LP: * Objective: Minimize/Maximize \\(f(\\mathbf{x}) = c_1x_1 + \\ldots + c_nx_n\\). * Constraints: Linear inequalities (\\(\\le, \\ge\\)) and/or equalities (\\(=\\)). * Non-negativity: \\(x_i \\ge 0\\) for all design variables. * Standard Form of an LP Problem: Essential for computational methods like the Simplex method. Requires: 1. Minimization Objective: Maximize \\(f(\\mathbf{x})\\) is converted to Minimize \\(-f(\\mathbf{x})\\). 2. Equality Constraints Only: * ‚Äú\\(\\le\\)‚Äù constraints become equalities by adding non-negative slack variables. * ‚Äú\\(\\ge\\)‚Äù constraints become equalities by subtracting non-negative surplus variables. 3. Non-negative Variables: All design variables (including slack/surplus) must be \\(\\ge 0\\). Unrestricted variables can be replaced by a difference of two non-negative variables. * Key Concepts Related to LP Solutions: * Feasible Region: A convex polyhedron (or polyhedral set). * Extreme Points (Vertices): If an optimal solution exists, it will be at at least one of the vertices of the feasible set. * Basic Feasible Solution (BFS): Corresponds to a vertex of the feasible region. * Active Constraints: Constraints satisfied as equalities at the optimum. * Multiple Optimal Solutions: Occur if the objective function contour is parallel to an active constraint boundary. * Unbounded Solution: Objective can be indefinitely improved. * Infeasible Problem: No solution satisfies all constraints. * Simplex Method: An iterative algebraic procedure for solving LP problems. It starts at a vertex, moves to an adjacent vertex with improved objective value, and continues until an optimal vertex is found.\nExample: * Profit Maximization Problem: Formulating a problem to maximize profit from producing two products, subject to resource limits, into both general and standard LP forms. Graphically finding the optimal production quantities at a vertex of the feasible region.\n\n\n\n\nCore Concepts: * Handling Nonlinear Programming (NLP): Lagrangian methods are crucial for solving general NLP problems where objective functions or constraints (or both) are nonlinear. * Lagrangian Function (\\(L\\)): Transforms a constrained optimization problem into an unconstrained one by incorporating constraints into the objective function using Lagrange multipliers. \\(L(\\mathbf{x}, \\mathbf{v}, \\mathbf{u}) = f(\\mathbf{x}) + \\sum_{k=1}^{p} v_k h_k(\\mathbf{x}) + \\sum_{j=1}^{m} u_j g_j(\\mathbf{x})\\) * \\(v_k\\): Lagrange multipliers for equality constraints \\(h_k(\\mathbf{x})=0\\), unrestricted in sign. * \\(u_j\\): Lagrange multipliers for inequality constraints \\(g_j(\\mathbf{x})\\le0\\), must be non-negative. * Karush‚ÄìKuhn‚ÄìTucker (KKT) Necessary Conditions: A set of conditions that must be satisfied at any local optimum point (\\(\\mathbf{x}^*\\)) of a constrained nonlinear problem. They are necessary, but not always sufficient, for a local minimum. 1. Gradient of the Lagrangian is Zero: \\(\\nabla L(\\mathbf{x}^*, \\mathbf{v}^*, \\mathbf{u}^*) = \\mathbf{0}\\). This means the objective gradient is a linear combination of active constraint gradients. 2. Feasibility: All original equality and inequality constraints must be satisfied at \\(\\mathbf{x}^*\\). 3. Complementary Slackness: \\(u_j^* g_j(\\mathbf{x}^*) = 0\\) for all inequality constraints. This implies: * If \\(g_j(\\mathbf{x}^*) &lt; 0\\) (inactive), then \\(u_j^* = 0\\). * If \\(u_j^* &gt; 0\\), then \\(g_j(\\mathbf{x}^*) = 0\\) (active). 4. Non-negativity of Inequality Multipliers: \\(u_j^* \\ge 0\\) for all \\(j\\). * Physical Meaning of Lagrange Multipliers: They represent the sensitivity of the optimal objective function value to a change in the constraint limit (often called ‚Äúshadow prices‚Äù). * Positive \\(u_j^*\\) for an active constraint implies tightening the constraint would increase the objective (for minimization). * Zero \\(u_j^*\\) for an inactive constraint means its limit doesn‚Äôt affect the optimum. * Second-Order Conditions (Brief Mention): KKT conditions are necessary. To confirm a local minimum, second-order sufficiency conditions involving the Hessian of the Lagrangian are needed, ensuring positive definiteness in feasible directions.\nExample: * Minimum Distance to Origin with a Linear Inequality Constraint: Applying KKT conditions to minimize \\(f(x_1, x_2) = x_1^2 + x_2^2\\) subject to \\(x_1 + x_2 - 1 \\le 0\\) and \\(x_1, x_2 \\ge 0\\). This example demonstrates how to systematically analyze different cases of active/inactive constraints to find the KKT points.\n\n\n\n\nCore Concepts: * Necessity: Numerical methods are essential for complex nonlinear constrained optimization problems with many variables, where analytical KKT solutions are impractical. * Iterative Process: All numerical methods use an iterative update formula: \\(\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + a_k \\mathbf{d}^{(k)}\\), where \\(a_k\\) is the step size and \\(\\mathbf{d}^{(k)}\\) is the search direction. * Constraint Handling Strategies: * Feasible Path: Maintains feasibility throughout iterations. * Infeasible Path: Allows temporary infeasibility, eventually guiding back to the feasible region. * Decomposition: Methods typically involve two subproblems: determining search direction and determining step size.\nKey Algorithms: 1. Sequential Linear Programming (SLP): * Idea: Approximates the nonlinear problem at each iteration with a linear program by using first-order Taylor series expansions for both objective and constraints. * LP Subproblem: Minimizes a linear approximation of \\(f(\\mathbf{x})\\) subject to linearized \\(h_k(\\mathbf{x})=0\\) and \\(g_j(\\mathbf{x})\\le0\\). * Move Limits: Crucial bounds (\\(-\\Delta_{iL}^{(k)} \\le d_i \\le \\Delta_{iU}^{(k)}\\)) are imposed on the design change \\(\\mathbf{d}\\) to ensure accuracy of the linear approximations. * Advantages/Drawbacks: Simple, uses established LP solvers, but can have slow convergence or oscillations. 2. Sequential Quadratic Programming (SQP): * Idea: Widely considered efficient for NLP. Solves a sequence of quadratic programming (QP) subproblems. * QP Subproblem: Minimizes a quadratic approximation of the Lagrangian function (or similar) subject to linear approximations of the constraints. * Hessian Approximation: Often uses quasi-Newton methods (e.g., BFGS) to approximate the Hessian matrix of the Lagrangian, ensuring it remains positive definite for a convex QP subproblem. * Advantages: Fast (superlinear) convergence, robustness for highly nonlinear problems. 3. Constrained Steepest-Descent (CSD) Method: * Idea: Finds a search direction by solving a QP subproblem that projects the steepest-descent direction onto the tangent hyperplane of active constraints. * QP Subproblem: Minimizes \\(\\nabla f(\\mathbf{x})^T \\mathbf{d} + \\frac{1}{2}\\mathbf{d}^T \\mathbf{d}\\) subject to linearized constraints. * Descent Functions (Merit Functions): Combine objective and constraint violation into a single function to determine step size \\(a_k\\) during line search. Pshenichny‚Äôs descent function is an example: \\(\\Phi(\\mathbf{x}, R) = f(\\mathbf{x}) + R V(\\mathbf{x})\\), where \\(V(\\mathbf{x})\\) is the maximum constraint violation and \\(R\\) is a penalty parameter.\nExample: * First Iteration of CSD Method: Illustrates finding the search direction and step size for a nonlinear problem using Pshenichny‚Äôs descent function, showing evaluation of functions, gradients, and checking the descent condition.\n\n\n\n\nCore Concepts: * Design optimization is an indispensable tool in modern engineering for improving performance, reducing costs, enhancing reliability, and ensuring safety.\nApplications in Mechanical Engineering Systems: 1. Structural Engineering: * Minimum-Weight Column/Beam Design: Minimizing material volume/mass subject to strength, stability (buckling), deflection, and geometric/manufacturing constraints. Numerical methods like SLP or SQP are commonly used due to nonlinearities. * Optimal Design using Standard Sections: Selecting components from predefined lists (e.g., W-shape steel sections) introduces discrete design variables (MV-OPT). These problems are solved using specialized methods such as genetic algorithms or branch-and-bound. 2. Mechanical System Design: * Helical Spring Optimization: Minimizing spring mass/volume subject to shear stress, deflection, surge frequency, and geometric constraints. This is a classic nonlinear programming problem. * Flywheel Design for Minimum Mass: Minimizing the volume of a flywheel subject to mass moment of inertia and von Mises stress requirements. 3. Manufacturing and Process Optimization: * Bolt Insertion/Welding Sequence (Traveling Salesman Problem - TSP): Minimizing travel distance/time for a robotic arm by determining the optimal sequence of operations. This is a discrete optimization problem often solved using Genetic Algorithms (GAs). GAs are based on biological evolution principles and are effective for discrete variable optimization problems. 4. Advanced and Cross-Disciplinary Areas: * Optimal Control: Some optimal control problems can be rephrased as optimization problems. * Robust Design: Designing systems to be insensitive to variations in manufacturing or operating conditions. Involves minimizing a ‚Äúloss function‚Äù that accounts for mean performance and variability. Taguchi methods often use orthogonal arrays to define sample points for robust design. * Reliability-Based Design Optimization (RBDO): Incorporates probabilistic aspects of loads and material properties to ensure a target probability of failure. * Meta-models / Response Surface Method (RSM): Creating simplified, explicit functions to approximate complex analysis models, reducing computational cost. RSM uses statistical methods like least squares to fit polynomial functions to sample data.",
    "crumbs": [
      "üìù Study Guide"
    ]
  },
  {
    "objectID": "documents/10-Study_Guide.html#course-overview",
    "href": "documents/10-Study_Guide.html#course-overview",
    "title": "Design Optimization: Study Guide",
    "section": "",
    "text": "Design optimization presents an organized approach to optimizing the design of engineering systems, illustrating basic concepts and procedures with examples to show their applicability to engineering design problems. The course material forms the basis for a first course on optimum design.",
    "crumbs": [
      "üìù Study Guide"
    ]
  },
  {
    "objectID": "documents/10-Study_Guide.html#lecture-1-introduction-to-design-optimization",
    "href": "documents/10-Study_Guide.html#lecture-1-introduction-to-design-optimization",
    "title": "Design Optimization: Study Guide",
    "section": "",
    "text": "Core Concepts: * Definition of Design Optimization: A systematic and organized approach to improving the design process by framing the design of a system as an optimization problem. It aims to maximize or minimize a specific performance measure while satisfying all other design requirements and constraints. This approach differs from the historical process where improvements were often considered only after substantial investments were recouped, leading to adequate rather than optimal systems. * Importance of Design Optimization: Critical for creating efficient, cost-effective, and competitive engineering systems by minimizing costs (e.g., material, manufacturing, energy) and maximizing desirable outcomes (e.g., performance, reliability, profit). * Overall Process of Designing Systems: An iterative process beginning with need identification, followed by defining specifications, preliminary design, detailed design (accelerated by optimization methods), and ultimately fabrication and use. * Engineering Design vs.¬†Engineering Analysis: * Engineering Analysis: Determining the response of a given system to a given input. * Engineering Design: Determining the parameters of a system to achieve desired performance under given inputs. * Conventional Design vs.¬†Optimum Design Process: Both are iterative, but conventional design aims for an ‚Äúacceptable‚Äù solution based on judgment, while optimum design systematically refines the design towards the ‚Äúbest‚Äù possible solution by formalizing the objective and constraints. * Optimum Design vs.¬†Optimal Control Problems: * Optimum Design: Focuses on finding the best physical parameters or configuration of a system. * Optimal Control: Determines the best way to operate or control an existing system over time. For example, a cruise control mechanism is an optimal control system. * Basic Terminology and Notation: Familiarity with linear algebra (vectors, matrices) and basic calculus (functions, derivatives) is essential. Key terms include: * Design Variables (\\(\\mathbf{x}\\)): Parameters adjusted by the designer. * Objective Function (\\(f(\\mathbf{x})\\)): Single performance measure to be optimized (minimized or maximized). * Constraints: Limitations or restrictions (e.g., material failure, demand, resources). * Equality Constraints (\\(h_k(\\mathbf{x}) = 0\\)) * Inequality Constraints (\\(g_j(\\mathbf{x}) \\le 0\\)) * Side Constraints: Bounds on individual design variables. * Feasible Set (S): Collection of all design points satisfying all constraints. * Optimum Solution (\\(\\mathbf{x}^*\\)): Design yielding the best objective value within the feasible set. * Active/Inactive Constraints: Constraints satisfied as an equality are active; otherwise, they are inactive. * Gradient (\\(\\nabla f(\\mathbf{x})\\)): Vector of partial derivatives, pointing in the direction of steepest increase. * Hessian Matrix (\\(\\mathbf{H}(\\mathbf{x})\\)): Matrix of second-order partial derivatives.",
    "crumbs": [
      "üìù Study Guide"
    ]
  },
  {
    "objectID": "documents/10-Study_Guide.html#lecture-2-optimum-design-problem-formulation",
    "href": "documents/10-Study_Guide.html#lecture-2-optimum-design-problem-formulation",
    "title": "Design Optimization: Study Guide",
    "section": "",
    "text": "Core Concepts: * Importance of Formulation: The proper definition and formulation of a problem can take over 50% of the total effort to solve it, as the optimum solution is only as good as its formulation. * Iterative Nature: Developing a proper formulation is often an iterative process in itself, requiring several revisions. * Five-Step Procedure for Problem Formulation: 1. Project/Problem Description: A clear descriptive statement outlining overall objectives and requirements. 2. Data and Information Collection: Gathering all necessary data (material properties, loads) and analysis expressions. 3. Definition of Design Variables (\\(\\mathbf{x}\\)): Identifying parameters that describe the system and can be freely assigned values by the designer. They should be as independent as possible and precisely defined, including units. 4. Optimization Criterion (Objective Function \\(f(\\mathbf{x})\\)): A single scalar measure to be minimized (e.g., mass, cost, energy) or maximized (e.g., efficiency, strength, profit). Multiple objectives lead to multiobjective problems. 5. Formulation of Constraints: Expressing all limitations (performance, geometric, resource, side constraints) mathematically as equalities (\\(h_k(\\mathbf{x}) = 0\\)) or inequalities (\\(g_j(\\mathbf{x}) \\le 0\\)). Side constraints are bounds on variables (e.g., \\(x_{iL} \\le x_i \\le x_{iU}\\)).\nExamples: * Minimum-Weight Tubular Column Design: Minimizing mass of a column subject to stress, buckling, and manufacturing constraints, with mean radius (\\(R\\)) and wall thickness (\\(t\\)) as design variables. * Maximize Volume of a Beer Mug: Maximizing volume subject to height, radius, and surface area limits, with radius (\\(R\\)) and height (\\(H\\)) as design variables.",
    "crumbs": [
      "üìù Study Guide"
    ]
  },
  {
    "objectID": "documents/10-Study_Guide.html#lecture-3-graphical-solution-method-and-basic-optimization-concepts",
    "href": "documents/10-Study_Guide.html#lecture-3-graphical-solution-method-and-basic-optimization-concepts",
    "title": "Design Optimization: Study Guide",
    "section": "",
    "text": "Core Concepts: * Applicability: Best suited for problems with two design variables, providing visual intuition into optimization concepts. * Graphical Solution Process: 1. Define Design Space: Establish a 2D coordinate system for the two design variables (\\(x_1, x_2\\)). 2. Plot Constraints and Identify Feasible Region: * Inequality constraints (\\(g_j(\\mathbf{x}) \\le 0\\)) define half-planes; equality constraints (\\(h_k(\\mathbf{x}) = 0\\)) define lines or curves. * The feasible region is the area where all constraints are simultaneously satisfied. 3. Plot Objective Function Contours: Draw lines/curves of constant objective function values (\\(f(\\mathbf{x}) = C\\)). These are called iso-cost or iso-profit lines. 4. Identify the Optimum Solution: ‚ÄúSlide‚Äù the objective function contour in the direction of improvement (decreasing for minimization, increasing for maximization) until it just touches the feasible region. The point(s) of last contact are the optimum. * Properties of Optimal Solutions in 2D: * The optimum is often at a vertex (corner point) of the feasible region. * If the objective function contour is parallel to an active constraint, there might be multiple optimal solutions along that edge. * Active Constraints: Constraints that are satisfied as equalities at the optimum point.\nExamples: * Maximize Volume of a Beer Mug: Graphically solved by plotting constraints on radius and height and sweeping volume contours. * Minimum-Weight Tubular Column Design: Graphically solved using given data to find optimal mean radius and thickness. * Profit Maximization Problem: A classic example to demonstrate plotting constraints and objective function contours to find maximum profit. * Use of Software: MATLAB and Mathematica can be used to plot functions and visualize graphical solutions.",
    "crumbs": [
      "üìù Study Guide"
    ]
  },
  {
    "objectID": "documents/10-Study_Guide.html#lecture-4-linear-programming-methods-for-optimum-design",
    "href": "documents/10-Study_Guide.html#lecture-4-linear-programming-methods-for-optimum-design",
    "title": "Design Optimization: Study Guide",
    "section": "",
    "text": "Core Concepts: * Definition of Linear Programming (LP): An optimization technique for problems where the objective function and all constraints are linear functions of the design variables. * General Mathematical Model for LP: * Objective: Minimize/Maximize \\(f(\\mathbf{x}) = c_1x_1 + \\ldots + c_nx_n\\). * Constraints: Linear inequalities (\\(\\le, \\ge\\)) and/or equalities (\\(=\\)). * Non-negativity: \\(x_i \\ge 0\\) for all design variables. * Standard Form of an LP Problem: Essential for computational methods like the Simplex method. Requires: 1. Minimization Objective: Maximize \\(f(\\mathbf{x})\\) is converted to Minimize \\(-f(\\mathbf{x})\\). 2. Equality Constraints Only: * ‚Äú\\(\\le\\)‚Äù constraints become equalities by adding non-negative slack variables. * ‚Äú\\(\\ge\\)‚Äù constraints become equalities by subtracting non-negative surplus variables. 3. Non-negative Variables: All design variables (including slack/surplus) must be \\(\\ge 0\\). Unrestricted variables can be replaced by a difference of two non-negative variables. * Key Concepts Related to LP Solutions: * Feasible Region: A convex polyhedron (or polyhedral set). * Extreme Points (Vertices): If an optimal solution exists, it will be at at least one of the vertices of the feasible set. * Basic Feasible Solution (BFS): Corresponds to a vertex of the feasible region. * Active Constraints: Constraints satisfied as equalities at the optimum. * Multiple Optimal Solutions: Occur if the objective function contour is parallel to an active constraint boundary. * Unbounded Solution: Objective can be indefinitely improved. * Infeasible Problem: No solution satisfies all constraints. * Simplex Method: An iterative algebraic procedure for solving LP problems. It starts at a vertex, moves to an adjacent vertex with improved objective value, and continues until an optimal vertex is found.\nExample: * Profit Maximization Problem: Formulating a problem to maximize profit from producing two products, subject to resource limits, into both general and standard LP forms. Graphically finding the optimal production quantities at a vertex of the feasible region.",
    "crumbs": [
      "üìù Study Guide"
    ]
  },
  {
    "objectID": "documents/10-Study_Guide.html#lecture-5-lagrangian-methods-for-optimum-design",
    "href": "documents/10-Study_Guide.html#lecture-5-lagrangian-methods-for-optimum-design",
    "title": "Design Optimization: Study Guide",
    "section": "",
    "text": "Core Concepts: * Handling Nonlinear Programming (NLP): Lagrangian methods are crucial for solving general NLP problems where objective functions or constraints (or both) are nonlinear. * Lagrangian Function (\\(L\\)): Transforms a constrained optimization problem into an unconstrained one by incorporating constraints into the objective function using Lagrange multipliers. \\(L(\\mathbf{x}, \\mathbf{v}, \\mathbf{u}) = f(\\mathbf{x}) + \\sum_{k=1}^{p} v_k h_k(\\mathbf{x}) + \\sum_{j=1}^{m} u_j g_j(\\mathbf{x})\\) * \\(v_k\\): Lagrange multipliers for equality constraints \\(h_k(\\mathbf{x})=0\\), unrestricted in sign. * \\(u_j\\): Lagrange multipliers for inequality constraints \\(g_j(\\mathbf{x})\\le0\\), must be non-negative. * Karush‚ÄìKuhn‚ÄìTucker (KKT) Necessary Conditions: A set of conditions that must be satisfied at any local optimum point (\\(\\mathbf{x}^*\\)) of a constrained nonlinear problem. They are necessary, but not always sufficient, for a local minimum. 1. Gradient of the Lagrangian is Zero: \\(\\nabla L(\\mathbf{x}^*, \\mathbf{v}^*, \\mathbf{u}^*) = \\mathbf{0}\\). This means the objective gradient is a linear combination of active constraint gradients. 2. Feasibility: All original equality and inequality constraints must be satisfied at \\(\\mathbf{x}^*\\). 3. Complementary Slackness: \\(u_j^* g_j(\\mathbf{x}^*) = 0\\) for all inequality constraints. This implies: * If \\(g_j(\\mathbf{x}^*) &lt; 0\\) (inactive), then \\(u_j^* = 0\\). * If \\(u_j^* &gt; 0\\), then \\(g_j(\\mathbf{x}^*) = 0\\) (active). 4. Non-negativity of Inequality Multipliers: \\(u_j^* \\ge 0\\) for all \\(j\\). * Physical Meaning of Lagrange Multipliers: They represent the sensitivity of the optimal objective function value to a change in the constraint limit (often called ‚Äúshadow prices‚Äù). * Positive \\(u_j^*\\) for an active constraint implies tightening the constraint would increase the objective (for minimization). * Zero \\(u_j^*\\) for an inactive constraint means its limit doesn‚Äôt affect the optimum. * Second-Order Conditions (Brief Mention): KKT conditions are necessary. To confirm a local minimum, second-order sufficiency conditions involving the Hessian of the Lagrangian are needed, ensuring positive definiteness in feasible directions.\nExample: * Minimum Distance to Origin with a Linear Inequality Constraint: Applying KKT conditions to minimize \\(f(x_1, x_2) = x_1^2 + x_2^2\\) subject to \\(x_1 + x_2 - 1 \\le 0\\) and \\(x_1, x_2 \\ge 0\\). This example demonstrates how to systematically analyze different cases of active/inactive constraints to find the KKT points.",
    "crumbs": [
      "üìù Study Guide"
    ]
  },
  {
    "objectID": "documents/10-Study_Guide.html#lecture-6-numerical-methods-for-constrained-optimum-design",
    "href": "documents/10-Study_Guide.html#lecture-6-numerical-methods-for-constrained-optimum-design",
    "title": "Design Optimization: Study Guide",
    "section": "",
    "text": "Core Concepts: * Necessity: Numerical methods are essential for complex nonlinear constrained optimization problems with many variables, where analytical KKT solutions are impractical. * Iterative Process: All numerical methods use an iterative update formula: \\(\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + a_k \\mathbf{d}^{(k)}\\), where \\(a_k\\) is the step size and \\(\\mathbf{d}^{(k)}\\) is the search direction. * Constraint Handling Strategies: * Feasible Path: Maintains feasibility throughout iterations. * Infeasible Path: Allows temporary infeasibility, eventually guiding back to the feasible region. * Decomposition: Methods typically involve two subproblems: determining search direction and determining step size.\nKey Algorithms: 1. Sequential Linear Programming (SLP): * Idea: Approximates the nonlinear problem at each iteration with a linear program by using first-order Taylor series expansions for both objective and constraints. * LP Subproblem: Minimizes a linear approximation of \\(f(\\mathbf{x})\\) subject to linearized \\(h_k(\\mathbf{x})=0\\) and \\(g_j(\\mathbf{x})\\le0\\). * Move Limits: Crucial bounds (\\(-\\Delta_{iL}^{(k)} \\le d_i \\le \\Delta_{iU}^{(k)}\\)) are imposed on the design change \\(\\mathbf{d}\\) to ensure accuracy of the linear approximations. * Advantages/Drawbacks: Simple, uses established LP solvers, but can have slow convergence or oscillations. 2. Sequential Quadratic Programming (SQP): * Idea: Widely considered efficient for NLP. Solves a sequence of quadratic programming (QP) subproblems. * QP Subproblem: Minimizes a quadratic approximation of the Lagrangian function (or similar) subject to linear approximations of the constraints. * Hessian Approximation: Often uses quasi-Newton methods (e.g., BFGS) to approximate the Hessian matrix of the Lagrangian, ensuring it remains positive definite for a convex QP subproblem. * Advantages: Fast (superlinear) convergence, robustness for highly nonlinear problems. 3. Constrained Steepest-Descent (CSD) Method: * Idea: Finds a search direction by solving a QP subproblem that projects the steepest-descent direction onto the tangent hyperplane of active constraints. * QP Subproblem: Minimizes \\(\\nabla f(\\mathbf{x})^T \\mathbf{d} + \\frac{1}{2}\\mathbf{d}^T \\mathbf{d}\\) subject to linearized constraints. * Descent Functions (Merit Functions): Combine objective and constraint violation into a single function to determine step size \\(a_k\\) during line search. Pshenichny‚Äôs descent function is an example: \\(\\Phi(\\mathbf{x}, R) = f(\\mathbf{x}) + R V(\\mathbf{x})\\), where \\(V(\\mathbf{x})\\) is the maximum constraint violation and \\(R\\) is a penalty parameter.\nExample: * First Iteration of CSD Method: Illustrates finding the search direction and step size for a nonlinear problem using Pshenichny‚Äôs descent function, showing evaluation of functions, gradients, and checking the descent condition.",
    "crumbs": [
      "üìù Study Guide"
    ]
  },
  {
    "objectID": "documents/10-Study_Guide.html#lecture-7-applications-of-design-optimization",
    "href": "documents/10-Study_Guide.html#lecture-7-applications-of-design-optimization",
    "title": "Design Optimization: Study Guide",
    "section": "",
    "text": "Core Concepts: * Design optimization is an indispensable tool in modern engineering for improving performance, reducing costs, enhancing reliability, and ensuring safety.\nApplications in Mechanical Engineering Systems: 1. Structural Engineering: * Minimum-Weight Column/Beam Design: Minimizing material volume/mass subject to strength, stability (buckling), deflection, and geometric/manufacturing constraints. Numerical methods like SLP or SQP are commonly used due to nonlinearities. * Optimal Design using Standard Sections: Selecting components from predefined lists (e.g., W-shape steel sections) introduces discrete design variables (MV-OPT). These problems are solved using specialized methods such as genetic algorithms or branch-and-bound. 2. Mechanical System Design: * Helical Spring Optimization: Minimizing spring mass/volume subject to shear stress, deflection, surge frequency, and geometric constraints. This is a classic nonlinear programming problem. * Flywheel Design for Minimum Mass: Minimizing the volume of a flywheel subject to mass moment of inertia and von Mises stress requirements. 3. Manufacturing and Process Optimization: * Bolt Insertion/Welding Sequence (Traveling Salesman Problem - TSP): Minimizing travel distance/time for a robotic arm by determining the optimal sequence of operations. This is a discrete optimization problem often solved using Genetic Algorithms (GAs). GAs are based on biological evolution principles and are effective for discrete variable optimization problems. 4. Advanced and Cross-Disciplinary Areas: * Optimal Control: Some optimal control problems can be rephrased as optimization problems. * Robust Design: Designing systems to be insensitive to variations in manufacturing or operating conditions. Involves minimizing a ‚Äúloss function‚Äù that accounts for mean performance and variability. Taguchi methods often use orthogonal arrays to define sample points for robust design. * Reliability-Based Design Optimization (RBDO): Incorporates probabilistic aspects of loads and material properties to ensure a target probability of failure. * Meta-models / Response Surface Method (RSM): Creating simplified, explicit functions to approximate complex analysis models, reducing computational cost. RSM uses statistical methods like least squares to fit polynomial functions to sample data.",
    "crumbs": [
      "üìù Study Guide"
    ]
  },
  {
    "objectID": "documents/08-Lecture_06.html",
    "href": "documents/08-Lecture_06.html",
    "title": "Lecture 6: Numerical Methods for Constrained Optimum Design",
    "section": "",
    "text": "Lecture 6: Numerical Methods for Constrained Optimum Design\nIn our previous lectures, we learned how to formulate an optimum design problem and, for specific cases, how to solve them analytically. We explored the graphical method for two-variable problems and Linear Programming for problems with linear objective functions and constraints. We also delved into Lagrangian methods, which provide the powerful Karush‚ÄìKuhn‚ÄìTucker (KKT) conditions for finding candidate optimal solutions for nonlinear problems.\nHowever, most real-world engineering design problems are complex, involving nonlinear objective functions and nonlinear constraints, often with many design variables. For such problems, analytical solutions to the KKT conditions can be exceedingly difficult or impossible to obtain. This is where numerical methods for constrained optimum design become indispensable. These methods are iterative and use computational algorithms to systematically search for the optimum solution.\nBy the end of this lecture, you will understand the fundamental concepts behind numerical methods for constrained optimization, learn about Sequential Linear Programming (SLP) and Sequential Quadratic Programming (SQP) as prominent algorithms, and understand the role of descent functions and the Constrained Steepest-Descent (CSD) method.\n\nBasic Concepts in Numerical Constrained Optimization\nAll numerical optimization methods, whether unconstrained or constrained, operate on an iterative principle. Starting from an initial design estimate, the design is progressively updated to reach an optimal point.\nThe general iterative update formula is: \\(\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\Delta \\mathbf{x}^{(k)}\\) or more commonly: \\(\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + a_k \\mathbf{d}^{(k)}\\)\nWhere: * \\(\\mathbf{x}^{(k)}\\) is the design variable vector at the \\(k\\)-th iteration. * \\(\\Delta \\mathbf{x}^{(k)}\\) is the total change in design in the \\(k\\)-th iteration. * \\(a_k\\) is the step size (a scalar that determines how far to move along the search direction). * \\(\\mathbf{d}^{(k)}\\) is the search direction (a vector indicating the path to move from the current design point).\nFor constrained optimization, the challenge lies in determining an appropriate search direction and step size that not only improves the objective function but also respects all the constraints. There are generally two strategies for handling constraints:\n\nFeasible Path Strategy: Start from a feasible point and generate a sequence of designs that remain feasible throughout the iterations. When a constraint boundary is encountered, the search direction is modified to travel along a tangent to the boundary or deflects into the feasible region.\nInfeasible Path Strategy: Allow iterations to temporarily move into the infeasible region, with the algorithms designed to eventually guide the search back to the feasible set, ideally converging to an optimum on the constraint boundary.\n\nMost numerical algorithms for constrained problems decompose the solution process into two subproblems: 1. Search Direction Determination: Calculates a vector \\(\\mathbf{d}^{(k)}\\) based on the current design and the values and gradients of the problem functions. 2. Step Size Determination: Calculates a scalar \\(a_k\\) by performing a one-dimensional search (line search) along the direction \\(\\mathbf{d}^{(k)}\\) to optimize a certain ‚Äúdescent function‚Äù.\n\n\nSequential Linear Programming (SLP)\nSequential Linear Programming (SLP) methods are an intuitive approach to solving nonlinear programming (NLP) problems by iteratively approximating the original problem with a sequence of linear programs.\nThe basic idea is as follows: 1. Start with an initial design \\(\\mathbf{x}^{(k)}\\). 2. Linearize the nonlinear objective function and all nonlinear constraints around the current design point using a first-order Taylor series expansion. This creates a linear approximation of the problem. 3. Formulate and solve this linearized problem as a Linear Programming (LP) subproblem to find a direction of design change, \\(\\mathbf{d}^{(k)}\\). 4. Update the design using the calculated direction and a suitable step size. 5. Repeat the process until convergence.\n\nLP Subproblem Formulation\nAt each iteration \\(k\\), the SLP algorithm defines an LP subproblem to find a design change \\(\\mathbf{d}^{(k)} = \\Delta \\mathbf{x}^{(k)}\\). If the original problem is to minimize \\(f(\\mathbf{x})\\) subject to \\(h_p(\\mathbf{x})=0\\) and \\(g_j(\\mathbf{x})\\le0\\):\nMinimize \\(f(\\mathbf{x}^{(k)}) + \\nabla f(\\mathbf{x}^{(k)})^T \\mathbf{d}\\) (this is a linear approximation of \\(f(\\mathbf{x}^{(k)} + \\mathbf{d})\\))\nSubject to: * \\(h_p(\\mathbf{x}^{(k)}) + \\nabla h_p(\\mathbf{x}^{(k)})^T \\mathbf{d} = 0\\), for \\(p = 1, \\ldots, p_{total}\\) (linearized equality constraints) * \\(g_j(\\mathbf{x}^{(k)}) + \\nabla g_j(\\mathbf{x}^{(k)})^T \\mathbf{d} \\le 0\\), for \\(j = 1, \\ldots, m_{total}\\) (linearized inequality constraints) * Move Limits: To ensure that the linear approximations are reasonably accurate, explicit bounds (called move limits) are placed on the design changes \\(\\mathbf{d}\\). \\(-\\Delta_{iL}^{(k)} \\le d_i \\le \\Delta_{iU}^{(k)}\\), for \\(i = 1, \\ldots, n\\) These limits are usually a fraction of the current design variable values. The variables \\(d_i\\) are typically allowed to be positive or negative (free in sign) and are converted to standard LP form (non-negative) by replacing \\(d_i = d_i^+ - d_i^-\\) with \\(d_i^+, d_i^- \\ge 0\\).\n\n\nSLP Algorithm Steps (Simplified)\n\nInitialize: Estimate a starting design \\(\\mathbf{x}^{(0)}\\), set iteration counter \\(k=0\\), and specify convergence parameters \\(\\epsilon_1, \\epsilon_2\\).\nEvaluate: Calculate the objective function and all constraint functions and their gradients at \\(\\mathbf{x}^{(k)}\\).\nFormulate LP Subproblem: Select appropriate move limits and construct the LP subproblem as described above.\nSolve LP Subproblem: Convert the LP subproblem to standard form (if necessary) and solve it for \\(\\mathbf{d}^{(k)}\\).\nCheck Convergence: If constraints are feasible within \\(\\epsilon_1\\) and the magnitude of \\(\\mathbf{d}^{(k)}\\) is below \\(\\epsilon_2\\), stop.\nUpdate Design: \\(\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\mathbf{d}^{(k)}\\). Set \\(k = k+1\\) and go to Step 2.\n\nAdvantages and Drawbacks: SLP is conceptually simple and uses well-established LP solvers. However, it can suffer from slow convergence or oscillations if the linear approximations are poor or if the move limits are not chosen carefully.\n\n\n\nSequential Quadratic Programming (SQP)\nSequential Quadratic Programming (SQP) methods are widely recognized as among the most robust and efficient algorithms for solving general nonlinear constrained optimization problems. They are based on solving a sequence of quadratic programming (QP) subproblems, which naturally incorporate second-order information about the problem functions.\nThe core idea of SQP is a generalization of Newton‚Äôs method for unconstrained optimization to constrained problems. At each iteration, an SQP method essentially performs two main steps:\n\nSearch Direction Calculation: Determine a search direction \\(\\mathbf{d}^{(k)}\\) by solving a Quadratic Programming (QP) subproblem. This subproblem involves minimizing a quadratic approximation of the Lagrangian function (or a similar augmented function) subject to linear approximations of the original constraints.\nStep Size Calculation: Perform a line search along the calculated direction \\(\\mathbf{d}^{(k)}\\) to find an optimum step size \\(a_k\\), typically by minimizing a special descent function (also called a merit function).\n\n\nQP Subproblem Formulation\nThe QP subproblem at iteration \\(k\\) (superscript \\(k\\) omitted for simplicity) is typically formulated as:\nMinimize \\(\\nabla f(\\mathbf{x})^T \\mathbf{d} + \\frac{1}{2}\\mathbf{d}^T \\mathbf{H} \\mathbf{d}\\)\nSubject to: * \\(\\nabla h_p(\\mathbf{x})^T \\mathbf{d} + h_p(\\mathbf{x}) = 0\\), for \\(p = 1, \\ldots, p_{total}\\) (linearized equality constraints) * \\(\\nabla g_j(\\mathbf{x})^T \\mathbf{d} + g_j(\\mathbf{x}) \\le 0\\), for \\(j = 1, \\ldots, m_{total}\\) (linearized inequality constraints)\nWhere: * \\(\\nabla f(\\mathbf{x})\\) is the gradient of the objective function. * \\(\\mathbf{H}\\) is the Hessian matrix of the Lagrangian function \\(\\nabla^2 L(\\mathbf{x}, \\mathbf{v}, \\mathbf{u})\\) or, more commonly, an approximation of it. * The terms \\(h_p(\\mathbf{x})\\) and \\(g_j(\\mathbf{x})\\) in the constraints are the current violations/values of the constraints.\nKey features of the QP subproblem: * The QP subproblem is typically designed to be strictly convex, which ensures that if a solution exists, it is global and unique within the subproblem. * Solving the QP subproblem not only yields the search direction \\(\\mathbf{d}\\) but also provides the Lagrange multipliers for the constraints. These multipliers are essential for the line search and for updating the Hessian approximation.\n\n\nQuasi-Newton Hessian Approximation\nCalculating the exact Hessian matrix of the Lagrangian function can be computationally expensive or difficult for many engineering problems. To overcome this, SQP methods often employ quasi-Newton methods (like BFGS) to approximate the Hessian matrix. These methods use only first-order derivative information (gradients of the Lagrangian function at successive points) to build and update the Hessian approximation iteratively.\nA crucial aspect is that the approximated Hessian matrix \\(\\mathbf{H}\\) is kept positive definite throughout the iterations. This ensures that the QP subproblem remains strictly convex, guaranteeing a unique and descent search direction.\nAdvantages of SQP: * Fast Convergence: SQP methods typically exhibit a superlinear rate of convergence, meaning they converge very rapidly near the optimum. * Robustness: They can handle highly nonlinear problems with many variables and constraints. * Generality: Applicable to problems with both equality and inequality constraints. * The QP subproblem is related to the KKT conditions for the original problem.\n\n\n\nDescent Functions for Constrained Optimization\nIn constrained optimization, simply reducing the objective function is not enough; the feasibility of the design must also be considered. Descent functions (also known as merit functions) combine the objective function and constraint violation into a single scalar function that can be minimized during the line search to determine the step size \\(a_k\\).\nA commonly used descent function is Pshenichny‚Äôs descent function: \\(\\Phi(\\mathbf{x}, R) = f(\\mathbf{x}) + R V(\\mathbf{x})\\)\nWhere: * \\(f(\\mathbf{x})\\) is the objective function. * \\(V(\\mathbf{x})\\) is the maximum constraint violation, defined as: \\(V(\\mathbf{x}) = \\max \\{0, |h_k(\\mathbf{x})| \\text{ for all } k, g_j(\\mathbf{x}) \\text{ for all } j\\}\\) (for \\(g_j(\\mathbf{x}) \\le 0\\)) Basically, \\(V(\\mathbf{x})\\) is 0 if all constraints are satisfied, and positive if any constraint is violated, equal to the magnitude of the largest violation. * \\(R\\) is a large positive penalty parameter. It ensures that constraint violations are heavily penalized, encouraging the algorithm to move towards the feasible region or reduce violations.\nThe line search then aims to find \\(a_k\\) that minimizes \\(\\Phi(\\mathbf{x}^{(k)} + a \\mathbf{d}^{(k)}, R)\\). The penalty parameter \\(R\\) is typically adjusted during the optimization process; if constraint violations are not improving, \\(R\\) is increased.\n\n\nThe Constrained Steepest-Descent (CSD) Method\nThe Constrained Steepest-Descent (CSD) method is a specific type of constrained optimization algorithm that modifies the steepest-descent direction to account for constraints. It is related to SQP in that it also involves solving a QP subproblem.\nThe philosophy of CSD is to find a search direction that is essentially the projection of the steepest-descent direction for the objective function onto the tangent hyperplane of the active constraints. This ensures that, for a small step, the design stays as close as possible to the active constraint boundaries while still improving the objective function.\nThe QP subproblem in the CSD method to determine the search direction \\(\\mathbf{d}\\) is formulated as: Minimize \\(\\nabla f(\\mathbf{x})^T \\mathbf{d} + \\frac{1}{2}\\mathbf{d}^T \\mathbf{d}\\) (this is similar to minimizing the magnitude of \\(\\mathbf{d}\\) while pushing it in the negative gradient direction)\nSubject to: * \\(\\nabla h_p(\\mathbf{x})^T \\mathbf{d} + h_p(\\mathbf{x}) = 0\\) (linearized equality constraints) * \\(\\nabla g_j(\\mathbf{x})^T \\mathbf{d} + g_j(\\mathbf{x}) \\le 0\\) (linearized inequality constraints)\nThe term \\(\\frac{1}{2}\\mathbf{d}^T \\mathbf{d}\\) in the objective is equivalent to minimizing \\(\\|\\mathbf{d}\\|^2\\), which means finding the shortest vector \\(\\mathbf{d}\\) that satisfies the linearized constraints and has the greatest component in the direction of \\(-\\nabla f(\\mathbf{x})\\).\n\nCSD Algorithm Steps (Summarized from)\n\nInitialize: Set initial design \\(\\mathbf{x}^{(0)}\\), iteration counter \\(k=0\\), penalty parameter \\(R_0\\), and convergence parameters \\(\\epsilon_1, \\epsilon_2\\).\nEvaluate: Calculate objective, constraint functions, and their gradients at \\(\\mathbf{x}^{(k)}\\).\nDefine and Solve QP Subproblem: Construct and solve the QP subproblem for the search direction \\(\\mathbf{d}^{(k)}\\) and Lagrange multipliers \\(\\mathbf{u}^{(k)}\\) and \\(\\mathbf{v}^{(k)}\\).\nCheck Convergence: If \\(\\|\\mathbf{d}^{(k)}\\| \\le \\epsilon_2\\) and the point is feasible, stop.\nStep Size Determination: Find an acceptable step size \\(a_k\\) by performing a line search on Pshenichny‚Äôs descent function \\(\\Phi(\\mathbf{x}, R)\\) along \\(\\mathbf{d}^{(k)}\\).\nUpdate Design: \\(\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + a_k \\mathbf{d}^{(k)}\\).\nUpdate Penalty Parameter: If the maximum constraint violation has not improved, increase \\(R\\).\nSet \\(k = k+1\\) and go to Step 2.\n\nThe CSD algorithm has been proven to converge to a KKT point for the general constrained optimization problem. It is a robust method, and its rate of convergence can be improved by including second-order information in the QP subproblem.\n\n\n\nSolved Example: Constrained Steepest-Descent Method (First Iteration)\nLet‚Äôs illustrate the first iteration of the CSD method for a nonlinear constrained problem.\nProblem Statement: Minimize the function \\(f(x_1, x_2) = x_1^2 + x_2^2 - 3x_1x_2\\) Subject to: \\(g_1(x_1, x_2) = \\frac{1}{6}x_1^2 + \\frac{1}{6}x_2^2 - 1 \\le 0\\) \\(g_2(x_1, x_2) = -x_1 \\le 0 \\quad (\\text{i.e., } x_1 \\ge 0)\\) \\(g_3(x_1, x_2) = -x_2 \\le 0 \\quad (\\text{i.e., } x_2 \\ge 0)\\)\nInitial Data: * Starting design \\(\\mathbf{x}^{(0)} = (1, 1)\\). * Penalty parameter \\(R_0 = 10\\). * Step reduction parameter \\(\\gamma = 0.5\\) (used in the descent condition, as per source example).\n\nIteration 1 (k = 0)\nStep 1: Evaluate Cost and Constraint Functions and Their Gradients at \\(\\mathbf{x}^{(0)} = (1, 1)\\)\n\nObjective Function: \\(f(1, 1) = (1)^2 + (1)^2 - 3(1)(1) = 1 + 1 - 3 = -1\\).\nConstraint Functions: \\(g_1(1, 1) = \\frac{1}{6}(1)^2 + \\frac{1}{6}(1)^2 - 1 = \\frac{1}{6} + \\frac{1}{6} - 1 = \\frac{2}{6} - 1 = \\frac{1}{3} - 1 = -\\frac{2}{3} \\le 0\\) (inactive). \\(g_2(1, 1) = -1 \\le 0\\) (inactive). \\(g_3(1, 1) = -1 \\le 0\\) (inactive).\nGradients: \\(\\nabla f(\\mathbf{x}) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2} \\right)^T = (2x_1 - 3x_2, 2x_2 - 3x_1)^T\\). At \\(\\mathbf{x}^{(0)} = (1, 1)\\), \\(\\nabla f(1, 1) = (2(1) - 3(1), 2(1) - 3(1))^T = (-1, -1)^T\\). \\(\\nabla g_1(\\mathbf{x}) = \\left( \\frac{\\partial g_1}{\\partial x_1}, \\frac{\\partial g_1}{\\partial x_2} \\right)^T = \\left( \\frac{1}{3}x_1, \\frac{1}{3}x_2 \\right)^T\\). At \\(\\mathbf{x}^{(0)} = (1, 1)\\), \\(\\nabla g_1(1, 1) = \\left( \\frac{1}{3}, \\frac{1}{3} \\right)^T\\). \\(\\nabla g_2(\\mathbf{x}) = (-1, 0)^T\\). \\(\\nabla g_3(\\mathbf{x}) = (0, -1)^T\\).\n\nStep 2: Define and Solve the QP Subproblem for the Search Direction \\(\\mathbf{d}^{(0)}\\)\nSince all constraints are inactive at \\(\\mathbf{x}^{(0)}\\), the QP subproblem simplifies to an unconstrained minimization: Minimize \\(\\nabla f(\\mathbf{x}^{(0)})^T \\mathbf{d} + \\frac{1}{2}\\mathbf{d}^T \\mathbf{I} \\mathbf{d}\\) Minimize \\((-1)d_1 + (-1)d_2 + \\frac{1}{2}(d_1^2 + d_2^2)\\)\nTo find the minimum, we set the partial derivatives with respect to \\(d_1\\) and \\(d_2\\) to zero: \\(\\frac{\\partial}{\\partial d_1} (-d_1 - d_2 + \\frac{1}{2}d_1^2 + \\frac{1}{2}d_2^2) = -1 + d_1 = 0 \\Rightarrow d_1 = 1\\). \\(\\frac{\\partial}{\\partial d_2} (-d_1 - d_2 + \\frac{1}{2}d_1^2 + \\frac{1}{2}d_2^2) = -1 + d_2 = 0 \\Rightarrow d_2 = 1\\).\nSo, the search direction is \\(\\mathbf{d}^{(0)} = (1, 1)^T\\). The Lagrange multipliers for the constraints from this subproblem are all zero, as none are active.\nStep 3: Step Size Determination using Pshenichny‚Äôs Descent Function\nWe need to find an acceptable step size \\(a_0\\) along \\(\\mathbf{d}^{(0)}\\). * Current Descent Function Value: \\(\\Phi_0 = f(\\mathbf{x}^{(0)}) + R_0 V(\\mathbf{x}^{(0)})\\). * \\(V(\\mathbf{x}^{(0)}) = \\max(0, g_1(1,1), g_2(1,1), g_3(1,1)) = \\max(0, -2/3, -1, -1) = 0\\). * \\(\\Phi_0 = -1 + 10(0) = -1\\). * Descent Condition Parameter: \\(b_0 = \\gamma \\|\\mathbf{d}^{(0)}\\|^2 = 0.5 \\left( (1)^2 + (1)^2 \\right) = 0.5(2) = 1\\). * Trial Step Sizes: We use \\(t_j = (0.5)^j\\) for \\(j=0, 1, 2, \\ldots\\).\n**Trial 1: $t_0 = 1$**\n*   **Trial Design**: $\\mathbf{x}^{(1,0)} = \\mathbf{x}^{(0)} + t_0 \\mathbf{d}^{(0)} = (1,1) + 1(1,1) = (2,2)$.\n*   **Evaluate Functions at $\\mathbf{x}^{(1,0)} = (2,2)$**:\n    $f(2,2) = (2)^2 + (2)^2 - 3(2)(2) = 4 + 4 - 12 = -4$.\n    $g_1(2,2) = \\frac{1}{6}(2)^2 + \\frac{1}{6}(2)^2 - 1 = \\frac{8}{6} - 1 = \\frac{4}{3} - 1 = \\frac{1}{3}$.\n    Since $g_1(2,2) = 1/3 &gt; 0$, this constraint is violated.\n*   **Maximum Constraint Violation**: $V(\\mathbf{x}^{(1,0)}) = \\max(0, 1/3, -2, -2) = 1/3$.\n*   **Descent Function Value**: $\\Phi_{1,0} = f(\\mathbf{x}^{(1,0)}) + R_0 V(\\mathbf{x}^{(1,0)}) = -4 + 10(1/3) = -4 + 10/3 = -\\frac{12}{3} + \\frac{10}{3} = -\\frac{2}{3} \\approx -0.667$.\n*   **Check Descent Condition**: $\\Phi_{1,0} \\le \\Phi_0 - t_0 b_0$.\n    $-\\frac{2}{3} \\le -1 - 1(1) = -2$. This inequality is **False** ($-0.667 \\not\\le -2$). So, $t_0=1$ is not acceptable.\n\n**Trial 2: $t_1 = 0.5$**\n*   **Trial Design**: $\\mathbf{x}^{(1,1)} = \\mathbf{x}^{(0)} + t_1 \\mathbf{d}^{(0)} = (1,1) + 0.5(1,1) = (1.5, 1.5)$.\n*   **Evaluate Functions at $\\mathbf{x}^{(1,1)} = (1.5, 1.5)$**:\n    $f(1.5, 1.5) = (1.5)^2 + (1.5)^2 - 3(1.5)(1.5) = 2.25 + 2.25 - 6.75 = -2.25$.\n    $g_1(1.5, 1.5) = \\frac{1}{6}(1.5)^2 + \\frac{1}{6}(1.5)^2 - 1 = \\frac{1}{6}(2.25) + \\frac{1}{6}(2.25) - 1 = \\frac{4.5}{6} - 1 = 0.75 - 1 = -0.25 \\le 0$ (inactive).\n*   **Maximum Constraint Violation**: $V(\\mathbf{x}^{(1,1)}) = \\max(0, -0.25, -1.5, -1.5) = 0$.\n*   **Descent Function Value**: $\\Phi_{1,1} = f(\\mathbf{x}^{(1,1)}) + R_0 V(\\mathbf{x}^{(1,1)}) = -2.25 + 10(0) = -2.25$.\n*   **Check Descent Condition**: $\\Phi_{1,1} \\le \\Phi_0 - t_1 b_0$.\n    $-2.25 \\le -1 - 0.5(1) = -1.5$. This inequality is **True**. So, $t_1=0.5$ is acceptable.\nStep 4 & 5: Update Design and Set Next Iteration * The acceptable step size is \\(a_0 = 0.5\\). * The new design is \\(\\mathbf{x}^{(1)} = (1.5, 1.5)\\). * Set \\(k=1\\) and proceed to the next iteration.\nThis concludes the first iteration of the Constrained Steepest-Descent method, moving the design from \\((1,1)\\) to \\((1.5, 1.5)\\), reducing the objective function value from -1 to -2.25, and maintaining feasibility.\nThis concludes our lecture on Numerical Methods for Constrained Optimum Design. We have covered fundamental concepts and two of the most popular algorithms, SLP and SQP, along with the CSD method. In the next lecture, we will explore various applications of design optimization in real-world engineering scenarios.",
    "crumbs": [
      "üìñ Lectures",
      "Lecture 6: Numerical Methods"
    ]
  },
  {
    "objectID": "documents/06-Lecture_04.html",
    "href": "documents/06-Lecture_04.html",
    "title": "Lecture 4: Linear Programming Methods for Optimum Design",
    "section": "",
    "text": "Lecture 4: Linear Programming Methods for Optimum Design\nWelcome back to Design Optimization! In our previous lectures, we laid the groundwork by defining design optimization, learned how to rigorously formulate a design problem mathematically, and explored the intuitive Graphical Solution Method for problems with two design variables. Today, we‚Äôre going to dive into a powerful and widely applicable class of optimization problems: Linear Programming (LP).\nWhile the graphical method provided excellent visual insights into feasible regions and optimum points, it becomes impractical for problems involving more than two (or sometimes three) design variables. Linear Programming offers a structured approach to solve a specific type of optimization problem that arises frequently in engineering, economics, and operations research, and it can handle a large number of variables and constraints computationally.\nBy the end of this lecture, you will be able to identify a linear programming problem, understand its general and standard forms, grasp the fundamental concepts related to LP solutions, and conceptualize how these problems are solved.\n\nWhat is Linear Programming (LP)?\nLinear Programming (LP) deals with optimization problems where both the objective function and all the constraints are linear functions of the design variables.\nKey characteristics of an LP problem: * Linear Objective Function: The function to be minimized or maximized is a sum of terms, where each term is a constant multiplied by a single design variable. For example, \\(f(x_1, x_2, \\ldots, x_n) = c_1x_1 + c_2x_2 + \\ldots + c_nx_n\\). * Linear Constraints: All limitations on the design variables are expressed as linear equalities or inequalities. These can be of the form \\(a_1x_1 + a_2x_2 + \\ldots + a_nx_n \\le b\\), \\(a_1x_1 + a_2x_2 + \\ldots + a_nx_n \\ge b\\), or \\(a_1x_1 + a_2x_2 + \\ldots + a_nx_n = b\\). * Non-negativity of Variables: Design variables are often required to be non-negative (e.g., you cannot produce a negative amount of a product).\nLP problems are a subset of continuous variable smooth optimization problems.\n\n\nGeneral Mathematical Model for an LP Problem\nA general LP problem can be stated as:\nFind the design variable vector \\(\\mathbf{x} = (x_1, x_2, ..., x_n)\\)\nTo minimize (or maximize) the objective function \\(f(\\mathbf{x}) = c_1x_1 + c_2x_2 + \\ldots + c_nx_n\\)\nSubject to: * A set of linear inequality constraints: \\(a_{j1}x_1 + a_{j2}x_2 + \\ldots + a_{jn}x_n \\le b_j\\), for \\(j = 1, \\ldots, m_1\\) * A set of linear equality constraints: \\(a_{k1}x_1 + a_{k2}x_2 + \\ldots + a_{kn}x_n = b_k\\), for \\(k = 1, \\ldots, m_2\\) * A set of linear inequality constraints (often \\(\\ge\\) type, or sometimes simple bounds): \\(a_{l1}x_1 + a_{l2}x_2 + \\ldots + a_{ln}x_n \\ge b_l\\), for \\(l = 1, \\ldots, m_3\\) * Non-negativity restrictions on design variables: \\(x_i \\ge 0\\), for \\(i = 1, \\ldots, n\\)\nHere, \\(c_i\\), \\(a_{ji}\\), \\(a_{ki}\\), \\(a_{li}\\), \\(b_j\\), \\(b_k\\), and \\(b_l\\) are all known constants. The total number of constraints is \\(m = m_1 + m_2 + m_3\\).\n\n\nStandard Form of an LP Problem\nFor computational methods like the Simplex method, it is crucial to convert any LP problem into a standard form. This allows for a uniform algorithmic approach. The standard form usually requires:\n\nObjective Function: Always expressed as a minimization problem. If the original problem is a maximization, it can be converted by multiplying the objective function by -1.\n\nMaximize \\(f(\\mathbf{x})\\) is equivalent to Minimize \\(-f(\\mathbf{x})\\).\n\nConstraints: All functional constraints must be equality constraints.\n\n‚ÄúLess than or equal to‚Äù (\\(\\le\\)) inequalities: These are converted to equalities by adding a non-negative slack variable. A slack variable represents the ‚Äúunused‚Äù resource or the difference between the left and right sides of the inequality.\n\nExample: \\(2x_1 + 3x_2 \\le 10 \\quad \\Rightarrow \\quad 2x_1 + 3x_2 + s_1 = 10\\), with \\(s_1 \\ge 0\\).\n\n‚ÄúGreater than or equal to‚Äù (\\(\\ge\\)) inequalities: These are converted to equalities by subtracting a non-negative surplus variable. A surplus variable represents the ‚Äúexcess‚Äù above the minimum requirement.\n\nExample: \\(4x_1 - x_2 \\ge 5 \\quad \\Rightarrow \\quad 4x_1 - x_2 - s_2 = 5\\), with \\(s_2 \\ge 0\\).\n\n\nVariables: All design variables must be non-negative. If an original variable \\(x_i\\) can be negative (i.e., \\(x_i\\) is ‚Äúunrestricted in sign‚Äù), it can be replaced by the difference of two new non-negative variables: \\(x_i = x_i' - x_i''\\), where \\(x_i' \\ge 0\\) and \\(x_i'' \\ge 0\\).\n\n\n\nKey Concepts and Terminology in LP\nUnderstanding these terms is vital for LP:\n\nFeasible Region (or Feasible Set): This is the set of all design points \\(\\mathbf{x}\\) that satisfy all the constraints, including the non-negativity conditions. For LP problems, the feasible region is always a convex polyhedron. A convex polyhedron is a geometric shape (like a polygon in 2D or a solid in 3D) where for any two points inside the shape, the entire line segment connecting them is also inside the shape.\nExtreme Points (or Vertices): These are the ‚Äúcorner points‚Äù of the feasible region. A fundamental property of LP is that if an optimal solution exists, it will always be found at at least one of the vertices of the feasible set.\nBasic Feasible Solution (BFS): In the context of the Simplex method, a BFS corresponds to an extreme point of the feasible region. It is a solution to the system of equality constraints where a certain number of variables (equal to the number of constraints) are ‚Äúbasic‚Äù (non-zero) and the rest are ‚Äúnon-basic‚Äù (zero).\nActive Constraints: At an optimum solution, any constraint that is satisfied as an equality (\\(g_j(\\mathbf{x}^*) = 0\\) or \\(h_k(\\mathbf{x}^*) = 0\\)) is called an active constraint. For LP, the optimal solution almost always lies on the boundary where some constraints are active.\nMultiple Optimal Solutions: It is possible for an LP problem to have more than one optimal solution. This occurs when the objective function contour (isoprofit or isocost line) is parallel to one of the active constraints that forms the boundary of the feasible region. In such cases, any point on the line segment connecting two optimal vertices along that boundary is also an optimal solution.\nDegeneracy: A basic feasible solution is degenerate if one or more basic variables have a value of zero.\nUnbounded Solution: The objective function can be increased or decreased indefinitely without violating any constraints. This usually indicates a problem in formulation.\nInfeasible Problem: No design point satisfies all the constraints simultaneously, meaning the feasible region is empty.\n\n\n\nGraphical Solution of an LP (Revisit with LP Focus)\nThe graphical method, though limited to two variables, perfectly illustrates the core concepts of LP. 1. Since both the objective function and constraints are linear, when plotted, the constraints will form straight lines (or planes in 3D). 2. The feasible region, as mentioned, will be a convex polygon (or polyhedron). 3. The objective function contours will be a series of parallel straight lines. 4. To find the optimum, you ‚Äúpush‚Äù the objective function line in the direction of improvement (minimization or maximization) until it just touches the feasible region. This last point of contact will almost always be an extreme point (vertex) of the feasible region. If the objective function line is parallel to an entire edge of the feasible region, then all points on that edge are optimal.\n\n\nIntroduction to the Simplex Method\nFor LP problems with more than two variables, graphical methods are insufficient, and numerical algorithms are required. The most famous and widely used algorithm for solving LP problems is the Simplex Method, developed by George Dantzig in the 1940s.\nConceptually, the Simplex method is an iterative algebraic procedure that: 1. Starts at an initial basic feasible solution (a vertex of the feasible region). 2. Systematically moves from the current vertex to an adjacent vertex, along an edge of the feasible region, such that the objective function value improves (decreases for minimization, increases for maximization). 3. Continues this ‚Äúvertex-hopping‚Äù process until no further improvement in the objective function is possible, indicating that the optimal solution has been reached.\nThe method efficiently explores only the vertices of the feasible region, rather than testing every single point, making it highly effective even for problems with a vast number of potential solutions.\n\n\nSolved Example: Profit Maximization Problem\nLet‚Äôs use the profit maximization problem referenced in our course material (similar to Fig 3.5) to illustrate LP formulation and its graphical solution.\nProblem Description: A company produces two products, Product 1 and Product 2. The profit per unit for Product 1 is $40, and for Product 2 is $50. Production is limited by available resources. * The raw material for Product 1 is limited such that no more than 4 units of Product 1 can be produced. * The combined labor hours for producing both products are limited. Producing 2 units of Product 1 and 1 unit of Product 2 consumes 12 hours of labor. * Another resource constraint relates to a specialized machine. Producing Product 1 consumes 1 unit of this resource, while Product 2 consumes 3 units. The total available specialized machine resource can handle no more than 12 units. * The company wants to maximize its total profit.\nStep 1: Define Design Variables Let: * \\(x_1\\) = number of units of Product 1 to produce * \\(x_2\\) = number of units of Product 2 to produce\nStep 2: Optimization Criterion (Objective Function) Maximize total profit: Maximize \\(f(x_1, x_2) = 40x_1 + 50x_2\\)\nStep 3: Formulation of Constraints 1. Product 1 Raw Material Limit: \\(x_1 \\le 4\\) 2. Labor Hour Limit: \\(2x_1 + x_2 \\le 12\\) 3. Specialized Machine Resource Limit: \\(-x_1 + 3x_2 \\le 12\\) (Note: the example in the source uses \\(-x_1\\) for Product 1‚Äôs consumption of resource, which implies a resource contribution/generation rather than consumption or just an artifact of the problem formulation leading to such an inequality structure.) 4. Non-negativity Constraints: \\(x_1 \\ge 0, x_2 \\ge 0\\)\n\nSummary of LP Formulation (General Form):\nMaximize \\(f(x_1, x_2) = 40x_1 + 50x_2\\)\nSubject to: \\(g_1: x_1 \\le 4\\) \\(g_2: 2x_1 + x_2 \\le 12\\) \\(g_3: -x_1 + 3x_2 \\le 12\\) \\(x_1 \\ge 0, x_2 \\ge 0\\)\n\nConverting to Standard Form (for Simplex Method):\nFirst, convert the maximization problem to a minimization problem: Minimize \\(-f(x_1, x_2) = -40x_1 - 50x_2\\)\nNext, introduce slack variables for each \\(\\le\\) inequality constraint: \\(g_1: x_1 + s_1 = 4 \\quad (s_1 \\ge 0)\\) \\(g_2: 2x_1 + x_2 + s_2 = 12 \\quad (s_2 \\ge 0)\\) \\(g_3: -x_1 + 3x_2 + s_3 = 12 \\quad (s_3 \\ge 0)\\)\nAll variables \\(x_1, x_2, s_1, s_2, s_3\\) must be non-negative.\nSummary of LP Formulation (Standard Form):\nMinimize \\(-f(x_1, x_2) = -40x_1 - 50x_2\\)\nSubject to: \\(x_1 + s_1 = 4\\) \\(2x_1 + x_2 + s_2 = 12\\) \\(-x_1 + 3x_2 + s_3 = 12\\) \\(x_1, x_2, s_1, s_2, s_3 \\ge 0\\)\n\nGraphical Solution:\n\nPlot Design Space: Use \\(x_1\\) on the horizontal axis and \\(x_2\\) on the vertical axis. Focus on the first quadrant due to \\(x_1 \\ge 0, x_2 \\ge 0\\).\nPlot Constraints and Feasible Region:\n\n\\(x_1 \\le 4\\): A vertical line at \\(x_1 = 4\\). Feasible region is to the left.\n\\(2x_1 + x_2 \\le 12\\): Find intercepts (e.g., if \\(x_1=0, x_2=12\\); if \\(x_2=0, x_1=6\\)). Draw the line connecting \\((0,12)\\) and \\((6,0)\\). Feasible region is below this line.\n\\(-x_1 + 3x_2 \\le 12\\): Find intercepts (e.g., if \\(x_1=0, x_2=4\\); if \\(x_2=0, x_1=-12\\)). Draw the line connecting \\((0,4)\\) and \\((-12,0)\\). Feasible region is below this line.\n\\(x_1 \\ge 0, x_2 \\ge 0\\): Limits to the first quadrant.\n\nThe feasible region is the polygon formed by the intersection of all these inequalities. It will have vertices at \\((0,0)\\), \\((4,0)\\), and the intersection points of the constraint lines.\n\nIntersection of \\(x_1=4\\) and \\(2x_1 + x_2 = 12\\): \\(2(4) + x_2 = 12 \\Rightarrow 8 + x_2 = 12 \\Rightarrow x_2 = 4\\). Vertex at \\((4,4)\\).\nIntersection of \\(2x_1 + x_2 = 12\\) and \\(-x_1 + 3x_2 = 12\\): Multiply second equation by 2: \\(-2x_1 + 6x_2 = 24\\). Add to first equation: \\((2x_1 + x_2) + (-2x_1 + 6x_2) = 12 + 24 \\Rightarrow 7x_2 = 36 \\Rightarrow x_2 = 36/7 \\approx 5.14\\). Then \\(x_1 = 3x_2 - 12 = 3(36/7) - 12 = 108/7 - 84/7 = 24/7 \\approx 3.43\\). Vertex at \\((24/7, 36/7)\\).\nIntersection of \\(x_1=0\\) and \\(-x_1 + 3x_2 = 12\\): \\(3x_2 = 12 \\Rightarrow x_2 = 4\\). Vertex at \\((0,4)\\).\n\nThe vertices of the feasible region are approximately: \\((0,0)\\), \\((4,0)\\), \\((4,4)\\), \\((24/7, 36/7)\\), and \\((0,4)\\).\nPlot Objective Function Contours: The objective is to Maximize \\(P = 40x_1 + 50x_2\\). Draw lines for constant profit values (e.g., \\(40x_1 + 50x_2 = C\\)). As \\(C\\) increases, the lines move away from the origin. Let‚Äôs test some values:\n\nIf \\(C = 100\\): \\(40x_1 + 50x_2 = 100 \\Rightarrow 4x_1 + 5x_2 = 10\\).\nIf \\(C = 200\\): \\(40x_1 + 50x_2 = 200 \\Rightarrow 4x_1 + 5x_2 = 20\\).\nThe slope of these lines is \\(-40/50 = -4/5\\).\n\nIdentify the Optimum Solution: ‚ÄúSlide‚Äù the profit line in the direction of increasing profit (away from the origin). The last point it touches in the feasible region is the optimum. Let‚Äôs evaluate the objective function at each vertex:\n\nAt \\((0,0)\\): \\(P = 40(0) + 50(0) = 0\\)\nAt \\((4,0)\\): \\(P = 40(4) + 50(0) = 160\\)\nAt \\((4,4)\\): \\(P = 40(4) + 50(4) = 160 + 200 = 360\\)\nAt \\((24/7, 36/7)\\) (approx. (3.43, 5.14)): \\(P = 40(24/7) + 50(36/7) = (960 + 1800)/7 = 2760/7 \\approx 394.28\\)\nAt \\((0,4)\\): \\(P = 40(0) + 50(4) = 200\\)\n\nThe maximum profit is approximately \\(394.28** at the point **\\)x_1^* = 24/7 \\(** and **\\)x_2^* = 36/7 $.\nAt this optimum point, the constraints \\(2x_1 + x_2 = 12\\) and \\(-x_1 + 3x_2 = 12\\) are active.\n\nThis example clearly demonstrates how linear objective functions and linear constraints define a convex feasible region, and the optimum is found at one of its vertices.\nThis concludes our lecture on Linear Programming Methods. In our next session, we will explore another powerful analytical method: Lagrangian Methods for Optimum Design.",
    "crumbs": [
      "üìñ Lectures",
      "Lecture 4: Linear Programming"
    ]
  },
  {
    "objectID": "documents/04-Lecture_02.html",
    "href": "documents/04-Lecture_02.html",
    "title": "Lecture 2: Optimum Design Problem Formulation",
    "section": "",
    "text": "Lecture 2: Optimum Design Problem Formulation\nIn our last lecture, we established the fundamental concepts of design optimization and differentiated it from traditional design and analysis. Today, we delve into a crucial aspect of optimum design: Problem Formulation. This is the step where we translate a descriptive engineering design challenge into a precise mathematical optimization problem. A well-formulated problem is the foundation for finding the best possible solution, so understanding this process is absolutely vital for any aspiring design engineer.\nBy the end of this lecture, you will be able to translate a descriptive statement of a design problem into a mathematical statement for optimization, identifying and defining its key components: design variables, optimization criterion, and constraints.\n\nThe Problem Formulation Process\nFormulating an optimum design problem involves a systematic, step-by-step approach. It‚Äôs essentially about asking the right questions and expressing the answers in a rigorous mathematical language.\n\nStep 1: Project/Problem Description\nThe formulation process begins with a clear, descriptive statement of the project or problem. This statement, often provided by a project owner or sponsor, outlines the overall objectives and specific requirements that the system must meet. It‚Äôs about understanding what needs to be designed and why.\nExample: ‚ÄúDesign a new, lightweight casing for a portable electronic device. The casing must be robust enough to withstand a drop from 1 meter without damage, have a smooth external finish, and be manufacturable using injection molding. The primary goal is to minimize the total weight of the casing.‚Äù\n\n\nStep 2: Data and Information Collection\nOnce the problem is clearly described, the next step is to gather all necessary data and relevant analysis expressions. This includes material properties, external loads, environmental conditions, geometric limitations, manufacturing processes, and any equations or formulas needed to analyze the system‚Äôs performance (e.g., stress calculations, deflection formulas, volume, mass equations). Without accurate data, even a perfectly formulated problem cannot lead to a meaningful solution.\nExample (for the electronic device casing): * Material properties: Density (\\(\\rho\\)), Young‚Äôs modulus (E), Yield strength (\\(\\sigma_y\\)) for potential plastics. * Load conditions: Impact force from a 1-meter drop. * Manufacturing limits: Minimum wall thickness for injection molding. * Geometric space: Maximum external dimensions for the casing. * Equations: Formulas for calculating stress, deflection, and mass based on geometry.\n\n\nStep 3: Definition of Design Variables\nDesign variables are the fundamental parameters that the designer can change to influence the system‚Äôs performance. They are the ‚Äúknobs‚Äù that you, as the engineer, can adjust. These variables are typically represented as an n-dimensional vector, x = (x1, x2, ‚Ä¶, xn).\nKey considerations for defining design variables:\n\nIndependence: Ideally, design variables should be as independent of each other as possible. If variables are highly dependent, it can complicate the optimization process.\nCompleteness: The chosen design variables should fully define the system such that its performance can be evaluated.\nInfluence: They should be parameters that have a significant impact on the objective function and constraints.\n\nExample (for the electronic device casing): If the casing is a simple rectangular box, design variables could be: * x1 = length of the casing * x2 = width of the casing * x3 = height of the casing * x4 = wall thickness of the casing\n\n\nStep 4: Optimization Criterion (Objective Function)\nThe optimization criterion, or objective function, is a single scalar measure of the design‚Äôs performance that you want to either minimize or maximize. It quantifies the ‚Äúgoodness‚Äù of a design. It must be expressed as a function of the design variables, f(x).\nCommon objectives in mechanical engineering:\n\nMinimize: Mass, cost, energy consumption, stress, deflection, vibration, manufacturing time.\nMaximize: Efficiency, strength, stiffness, lifespan, profit, reliability, ride quality.\n\nSometimes, a problem may seem to have multiple objectives (e.g., minimize weight and minimize cost). Such problems are called multiobjective design optimization problems, and they require specialized techniques (which we will briefly touch upon later in the course). For now, we will focus on problems with a single, clearly defined objective.\nExample (for the electronic device casing): To minimize the total mass of the casing. Assuming a simple hollow rectangular box, the mass (M) could be approximated as: M = \\(\\rho\\) * (Volume of solid material) If the outer dimensions are \\(L_o\\), \\(W_o\\), \\(H_o\\) and wall thickness is \\(t\\): Volume = \\((L_o W_o H_o) - ((L_o - 2t)(W_o - 2t)(H_o - 2t))\\) So, \\(f(x_1, x_2, x_3, x_4)\\) = \\(\\rho \\times (x_1 x_2 x_3 - (x_1 - 2x_4)(x_2 - 2x_4)(x_3 - 2x_4))\\)\n\n\nStep 5: Formulation of Constraints\nConstraints are limitations or restrictions on the design variables or the system‚Äôs performance. They arise naturally in almost all engineering problems and ensure that the final design is practical, safe, and meets all functional requirements. Constraints are expressed mathematically as equalities or inequalities involving the design variables.\nTypes of constraints:\n\nPerformance Constraints: Related to the behavior of the system (e.g., stress must not exceed allowable stress, deflection must be less than a certain limit).\n\nInequality: \\(g_j(x) \\le 0\\)\nEquality: \\(h_k(x) = 0\\)\n\nGeometric Constraints: Limitations on dimensions (e.g., width must be less than length, minimum clearance).\nResource Constraints: Limitations on available materials, budget, manufacturing capacity.\nSide Constraints (Bounds on Variables): Simple upper and lower limits on each design variable.\n\n\\(x_{iL} \\le x_i \\le x_{iU}\\)\n\n\nIt is a common practice to convert all inequality constraints into the ‚Äúless than or equal to zero‚Äù form (\\(g_j(x) \\le 0\\)) for standardization. If a constraint is originally \\(G(x) \\ge C\\), it can be rewritten as \\(C - G(x) \\le 0\\).\nExample (for the electronic device casing):\n\nMaterial Strength Constraint: Maximum stress (\\(\\sigma_{max}\\)) must not exceed the material‚Äôs yield strength (\\(\\sigma_y\\)). This will depend on the impact load and geometry. \\(g_1(x_1, x_2, x_3, x_4) = \\sigma_{max}(x_1, x_2, x_3, x_4) - \\sigma_y \\le 0\\)\nDeflection Constraint: Maximum deflection (\\(\\delta_{max}\\)) must not exceed a specified limit (\\(\\delta_{allow}\\)). \\(g_2(x_1, x_2, x_3, x_4) = \\delta_{max}(x_1, x_2, x_3, x_4) - \\delta_{allow} \\le 0\\)\nWidth-to-thickness ratio: To avoid local buckling or ensure manufacturability. \\(g_3(x_2, x_4) = \\frac{x_2}{x_4} - \\text{ratio}_{max} \\le 0\\)\nSide Constraints (Bounds on Variables): \\(x_{1L} \\le x_1 \\le x_{1U}\\) (e.g., 50 mm \\(\\le x_1 \\le\\) 200 mm) \\(x_{2L} \\le x_2 \\le x_{2U}\\) (e.g., 30 mm \\(\\le x_2 \\le\\) 100 mm) \\(x_{3L} \\le x_3 \\le x_{3U}\\) (e.g., 10 mm \\(\\le x_3 \\le\\) 50 mm) \\(x_{4L} \\le x_4 \\le x_{4U}\\) (e.g., 1 mm \\(\\le x_4 \\le\\) 5 mm)\n\n\n\nGeneral Mathematical Model for Optimum Design\nCombining all these steps, a general mathematical model for an optimum design problem can be stated as:\nFind the design variable vector \\(\\mathbf{x} = (x_1, x_2, ..., x_n)\\)\nTo minimize (or maximize) the objective function \\(f(\\mathbf{x})\\)\nSubject to: * Equality constraints: \\(h_k(\\mathbf{x}) = 0\\), for \\(k = 1, ..., p\\) * Inequality constraints: \\(g_j(\\mathbf{x}) \\le 0\\), for \\(j = 1, ..., m\\) * Side constraints (bounds): \\(x_{iL} \\le x_i \\le x_{iU}\\), for \\(i = 1, ..., n\\)\nThe set of all design points x that satisfy all these constraints is known as the feasible set or feasible design space. Our goal is to find the point within this feasible set that gives the best (minimum or maximum) value for the objective function.\n\n\n\nSolved Examples\nLet‚Äôs apply this formulation process to a few engineering design problems.\n\nExample 1: Minimum-Weight Tubular Column Design\n\n\n\nMinimum-Weight Tubular Column Design\n\n\nStep 1: Project/Problem Description Design a minimum-mass tubular column of length \\(L\\) supporting an axial load \\(P\\) without buckling or overstressing. The column is fixed at the base and free at the top (a cantilever column).\nStep 2: Data and Information Collection * Given: Applied load \\(P\\), column length \\(L\\), modulus of elasticity \\(E\\), allowable stress \\(\\sigma_a\\), mass density \\(\\rho\\). * Formulas: * Buckling load for a cantilever column: \\(P_{cr} = \\frac{\\pi^2 E I}{4 L^2}\\) * Axial stress: \\(\\sigma = \\frac{P}{A}\\) * For a thin-walled tubular column with mean radius \\(R\\) and wall thickness \\(t\\): * Cross-sectional area: \\(A = 2\\pi Rt\\) * Moment of inertia: \\(I = \\pi R^3 t\\) * Mass: \\(M = \\rho L A = 2\\pi \\rho L R t\\)\nStep 3: Definition of Design Variables For this formulation, we define: * \\(x_1 = R\\) (mean radius of the column, in meters) * \\(x_2 = t\\) (wall thickness, in meters)\nStep 4: Optimization Criterion Minimize the total mass of the column: Minimize \\(f(x_1, x_2) = 2\\pi \\rho L x_1 x_2\\)\nStep 5: Formulation of Constraints 1. Stress Constraint: The axial stress must not exceed the allowable stress. \\(\\frac{P}{A} \\le \\sigma_a \\quad \\Rightarrow \\quad \\frac{P}{2\\pi x_1 x_2} \\le \\sigma_a\\) Rewriting as \\(g_j(\\mathbf{x}) \\le 0\\): \\(g_1(x_1, x_2) = \\frac{P}{2\\pi x_1 x_2 \\sigma_a} - 1 \\le 0\\) 2. Buckling Constraint: The applied load must not exceed the critical buckling load. \\(P \\le P_{cr} \\quad \\Rightarrow \\quad P \\le \\frac{\\pi^2 E (\\pi x_1^3 x_2)}{4 L^2}\\) Rewriting as \\(g_j(\\mathbf{x}) \\le 0\\): \\(g_2(x_1, x_2) = \\frac{4 P L^2}{\\pi^3 E x_1^3 x_2} - 1 \\le 0\\) 3. Manufacturing/Geometric Constraint: To ensure a thin-walled tube, the mean radius should be significantly larger than the thickness. A common rule of thumb is \\(R/t \\ge 10\\) or \\(R/t \\le 50\\). Let‚Äôs use \\(R/t \\le 50\\). \\(g_3(x_1, x_2) = \\frac{x_1}{x_2} - 50 \\le 0\\) 4. Side Constraints (Bounds on Design Variables): * Lower bound for radius: \\(x_1 \\ge R_{min}\\) (e.g., 0.01 m) * Upper bound for radius: \\(x_1 \\le R_{max}\\) (e.g., 0.4 m) * Lower bound for thickness: \\(x_2 \\ge t_{min}\\) (e.g., 0.001 m) * Upper bound for thickness: \\(x_2 \\le t_{max}\\) (e.g., 0.1 m)\nSummary of Formulation: Minimize \\(f(x_1, x_2) = 2\\pi \\rho L x_1 x_2\\) Subject to: \\(g_1(x_1, x_2) = \\frac{P}{2\\pi x_1 x_2 \\sigma_a} - 1 \\le 0\\) \\(g_2(x_1, x_2) = \\frac{4 P L^2}{\\pi^3 E x_1^3 x_2} - 1 \\le 0\\) \\(g_3(x_1, x_2) = \\frac{x_1}{x_2} - 50 \\le 0\\) \\(R_{min} \\le x_1 \\le R_{max}\\) \\(t_{min} \\le x_2 \\le t_{max}\\)\n\nExample 2: Maximize Volume of a Coffee Mug\nStep 1: Project/Problem Description Design a cylindrical Coffee mug (without a lid) to hold as much coffee as possible.\n\n\n\nCoffee mug\n\n\nStep 2: Data and Information Collection * The mug has a height \\(H\\) and a radius \\(R\\). * There are limitations on the mug‚Äôs dimensions and surface area. * Formulas: * Volume of a cylinder: \\(V = \\pi R^2 H\\) * Surface area of the sides: \\(A_{sides} = 2\\pi R H\\) (ignoring the bottom and handle area, as specified in a typical problem statement).\nStep 3: Definition of Design Variables * \\(x_1 = R\\) (radius of the mug, in cm) * \\(x_2 = H\\) (height of the mug, in cm)\nStep 4: Optimization Criterion Maximize the volume of the mug: Maximize \\(f(x_1, x_2) = \\pi x_1^2 x_2\\)\nStep 5: Formulation of Constraints 1. Maximum Height Constraint: The height of the mug should be no more than 20 cm. \\(x_2 \\le 20 \\quad \\Rightarrow \\quad g_1(x_1, x_2) = x_2 - 20 \\le 0\\) 2. Maximum Radius Constraint: The radius of the mug should be no more than 20 cm. \\(x_1 \\le 20 \\quad \\Rightarrow \\quad g_2(x_1, x_2) = x_1 - 20 \\le 0\\) 3. Minimum Radius Constraint: The mug must be at least 5 cm in radius. \\(x_1 \\ge 5 \\quad \\Rightarrow \\quad g_3(x_1, x_2) = 5 - x_1 \\le 0\\) 4. Surface Area Constraint: The surface area of the sides must be no greater than 900 cm\\(^2\\). \\(2\\pi R H \\le 900 \\quad \\Rightarrow \\quad 2\\pi x_1 x_2 \\le 900\\) Rewriting as \\(g_j(\\mathbf{x}) \\le 0\\): \\(g_4(x_1, x_2) = 2\\pi x_1 x_2 - 900 \\le 0\\) 5. Non-negativity Constraints (Implicit in bounds, but explicitly stated for completeness): \\(x_1 \\ge 0\\) \\(x_2 \\ge 0\\)\nSummary of Formulation: Maximize \\(f(x_1, x_2) = \\pi x_1^2 x_2\\) Subject to: \\(g_1(x_1, x_2) = x_2 - 20 \\le 0\\) \\(g_2(x_1, x_2) = x_1 - 20 \\le 0\\) \\(g_3(x_1, x_2) = 5 - x_1 \\le 0\\) \\(g_4(x_1, x_2) = 2\\pi x_1 x_2 - 900 \\le 0\\) \\(x_1 \\ge 0\\) \\(x_2 \\ge 0\\)\n\nThis concludes our discussion on optimum design problem formulation. Mastering this skill is fundamental, as it dictates the success of all subsequent optimization efforts. In the next lecture, we will explore the Graphical Solution Method and use it to visualize basic optimization concepts, especially for problems with two design variables.",
    "crumbs": [
      "üìñ Lectures",
      "Lecture 2: Problem Formulation"
    ]
  },
  {
    "objectID": "documents/02-about.html",
    "href": "documents/02-about.html",
    "title": " Design Optimization",
    "section": "",
    "text": "Introduction to Design Optimization: We‚Äôll begin by defining what design optimization is and why it‚Äôs so important.\nOptimum Design Problem Formulation:  Learn how to clearly define a design problem so you can find the best possible solution.\nGraphical Solution Method and Basic Optimization Concepts:  Understand core optimization ideas through visual examples and fundamental principles.\nLinear Programming Methods for Optimum Design:  Discover specific techniques for solving design problems using linear programming.\nLagrangian Methods for Optimum Design:  Explore another powerful method, Lagrangian techniques, to tackle design optimization challenges.\nNumerical Methods for Constrained Optimum Design:  Learn advanced numerical approaches for solving complex design problems that have specific limitations.\nApplications:  See how design optimization is applied in various real-world scenarios across different industries.",
    "crumbs": [
      "üìö Course Contents"
    ]
  },
  {
    "objectID": "documents/02-about.html#course-overview",
    "href": "documents/02-about.html#course-overview",
    "title": " Design Optimization",
    "section": "",
    "text": "Introduction to Design Optimization: We‚Äôll begin by defining what design optimization is and why it‚Äôs so important.\nOptimum Design Problem Formulation:  Learn how to clearly define a design problem so you can find the best possible solution.\nGraphical Solution Method and Basic Optimization Concepts:  Understand core optimization ideas through visual examples and fundamental principles.\nLinear Programming Methods for Optimum Design:  Discover specific techniques for solving design problems using linear programming.\nLagrangian Methods for Optimum Design:  Explore another powerful method, Lagrangian techniques, to tackle design optimization challenges.\nNumerical Methods for Constrained Optimum Design:  Learn advanced numerical approaches for solving complex design problems that have specific limitations.\nApplications:  See how design optimization is applied in various real-world scenarios across different industries.",
    "crumbs": [
      "üìö Course Contents"
    ]
  },
  {
    "objectID": "documents/03-Lecture_01.html",
    "href": "documents/03-Lecture_01.html",
    "title": "Lecture 1: Introduction to Design Optimization",
    "section": "",
    "text": "Lecture 1: Introduction to Design Optimization\nWelcome to Design Optimization! In this course, we will explore how to systematically make engineering systems better. This first lecture will lay the groundwork by defining what design optimization is, why it‚Äôs so critical in modern engineering, and how it differs from traditional design approaches and other related fields.\n\nWhat is Design Optimization?\nEngineering encompasses a variety of established activities, including analysis, design, fabrication, sales, research, and development of systems. Among these, the design of systems is a fundamental field in the engineering profession. Historically, the process of designing and fabricating complex systems has evolved over centuries, leading to the existence of remarkable structures like buildings, bridges, automobiles, and airplanes. However, this evolution has often been slow, time-consuming, and costly, frequently resulting in systems that were merely adequate rather than optimal. The procedure was often to design, fabricate, and use a system regardless of whether it was the best one, with improvements only being considered after substantial investments had been recouped.\nDesign optimization offers a systematic and organized approach to improving this process. It frames the design of a system as a problem of optimization, where a specific performance measure is maximized or minimized, while ensuring that all other design requirements and constraints are satisfied. Essentially, it helps engineers find the best possible system given a set of criteria and limitations.\nNumerical methods of optimization have been extensively developed and are widely used to achieve these better, more efficient designs. This course will focus on the practical design process, emphasizing the applicability of optimization concepts and methods to real-world engineering challenges, rather than delving into rigorous theoretical proofs.\n\n\nWhy is Design Optimization So Important?\nThe importance of design optimization has grown rapidly across various industries. In today‚Äôs competitive landscape, engineers are constantly challenged to create systems that are efficient and cost-effective without compromising their integrity.\n\nCompetitive Advantage: Optimizing designs allows companies to beat the competition by producing superior products or services at a lower cost.\nImproved Bottom Line: By minimizing costs (e.g., material, manufacturing, energy expenditure) or maximizing desirable outcomes (e.g., profit, performance, reliability, ride quality), optimization directly contributes to financial success.\nResource Efficiency: It helps in making the best use of available resources, preventing waste and ensuring sustainability.\nEnhanced Performance: Optimization leads to systems that are more efficient, reliable, durable, and perform better under various conditions.\nHandling Complexity: Modern engineering systems are increasingly complex. Optimization provides a structured way to manage this complexity and find solutions that would be difficult or impossible to achieve through trial-and-error.\n\n\n\nThe Overall Process of Designing Systems\nDesigning engineering systems is often a complex, interdisciplinary endeavor. It begins with the identification of a need and progresses through several organized steps:\n\nDefine Specifications: The first crucial step is to precisely define the specifications for the system. This often requires considerable interaction between the engineer and the project sponsor to quantify all system requirements.\nAnalysis of Options: The design process starts by analyzing various options. Subsystems and their components are identified, designed, and tested.\nDetailed Design with Optimization: This is where optimization methods significantly accelerate the process. For all promising design concepts, a detailed design for all subsystems is performed using an iterative process. Design parameters for the subsystems are identified, and system performance requirements are formulated. The goal is to design subsystems to maximize system worth or minimize a measure of cost. The outcome is a detailed description of the final design, presented in reports and drawings.\nFabrication and Use: The system is then fabricated and put into use.\n\nIt is important to understand that design is an iterative process. This means analyzing several trial designs one after another until an acceptable (or, in optimization, the best) design is obtained. Designers typically start with an initial trial design based on experience, intuition, or simple analyses. This trial design is then analyzed. In conventional design, the process terminates if the design is acceptable. In optimum design, the trial design is analyzed to determine if it is the ‚Äúbest,‚Äù implying it is cost-effective, efficient, reliable, and durable.\nExample: Designing a High-Rise Building or Passenger Car Consider the design of a high-rise building. This project involves architects, structural engineers, mechanical engineers, electrical engineers, environmental engineers, and construction management experts. Similarly, designing a passenger car requires cooperation among structural, mechanical, automotive, electrical, chemical, hydraulics design, and human factors engineers. In such interdisciplinary environments, the entire design project must often be broken down into several subproblems, each of which can be posed as a problem of optimum design.\n\n\nEngineering Design vs.¬†Engineering Analysis\nIt‚Äôs crucial to distinguish between engineering design and engineering analysis:\n\nEngineering Analysis: This activity involves determining the response of a given system to a given input. For example, if you have a bridge of known dimensions and materials, analysis would calculate the stresses, deflections, and natural frequencies under a specified load. This typically involves applying fundamental principles of physics and mathematics.\nEngineering Design: This activity focuses on determining the parameters of a system to achieve desired performance under given inputs. Using the bridge example, design would involve determining the dimensions (e.g., beam depths, column widths) and materials such as to safely support a given load, minimize material cost, and satisfy all relevant codes. Design is inherently a synthesis process, where choices are made to meet objectives.\n\nUnderstanding analysis methods is a prerequisite for design optimization. In this course, we will assume students understand basic analysis methods; equations for system analysis will be provided where necessary.\n\n\nConventional Design Process vs.¬†Optimum Design Process\nBoth conventional and optimum design methods are iterative. However, a key difference lies in how they approach the goal:\n\nConventional Design Process:\n\nBegins with data and an initial design estimate.\nThe system is analyzed.\nPerformance requirements are checked. If they are met, the design is accepted and the process stops.\nIf requirements are not met, the design is modified based on engineering judgment or simple calculations, and the process loops back to analysis.\nThis approach aims for an acceptable design.\n\nOptimum Design Process:\n\nIncludes an initial step (Block 0) where the problem is formulated as one of optimization. This means defining an objective function that precisely measures the merits of different designs (e.g., minimize cost, maximize efficiency).\nSimilar to conventional design, it requires data and an initial design estimate, followed by system analysis.\nInstead of just checking for acceptability, the design is systematically improved by an optimization algorithm to move towards the optimum value of the objective function while satisfying all constraints.\nThe process continues refining the design until the objective function can no longer be improved, leading to the ‚Äúbest‚Äù possible design.\n\n\nKey takeaway: The optimum design method formalizes the improvement process, leading to superior results compared to the trial-and-error nature of conventional design.\n\n\nOptimum Design vs.¬†Optimal Control Problems\nWhile both optimum design and optimal control involve making systems ‚Äúoptimal,‚Äù they address different aspects:\n\nOptimum Design: Focuses on determining the best physical parameters or configuration of a system to achieve a desired performance. The design variables (e.g., dimensions, material properties) are typically fixed values once determined.\n\nExample: Determining the optimal dimensions (radius, thickness) of a tubular column to minimize its mass while ensuring it doesn‚Äôt buckle or overstress under a given load. The goal is to find the best structure.\n\nOptimal Control: Deals with determining the best way to operate or control an existing system over time to achieve a desired output. The control variables are functions of time or state variables.\n\nExample: A cruise control mechanism in a passenger car. The system‚Äôs output (vehicle‚Äôs cruising speed) is known. The control mechanism‚Äôs job is to sense fluctuations in speed due to road conditions and adjust fuel injection (the control input) accordingly to maintain that constant speed. The goal is to find the best operation strategy.\n\n\nIt is important to note that while they are separate activities, some optimal control problems can be effectively transformed into optimum design problems and solved using the methods we will cover in this course. This highlights the broad power and applicability of optimum design techniques.\n\n\nBasic Terminology and Notation\nTo effectively understand and apply the methods of optimum design, familiarity with some basic mathematical terminology and notation is essential. This includes concepts from linear algebra (vectors, matrices, and their operations) and basic calculus (functions of single and multiple variables, derivatives).\nWe will use standard terminology throughout this course. Here are some key concepts you should be familiar with:\n\nDesign Variables (x): These are the parameters of the system that can be changed by the designer. They are usually represented as an n-dimensional vector x = (x1, x2, ‚Ä¶, xn). For example, in designing a beam, the width and depth could be design variables.\nObjective Function (f(x)): This is the single scalar measure of performance that you want to optimize (minimize or maximize). It depends on the design variables.\nConstraints: These are limitations or restrictions on the design variables or system performance. They arise naturally in engineering problems (e.g., material strength limits, available space, budget).\n\nEquality Constraints (hj(x) = 0): These must be satisfied exactly.\nInequality Constraints (gi(x) ‚â§ 0): These must be satisfied such that a certain value is less than or equal to a limit. Sometimes they are written as g_i(x) &gt;= 0, but for standardization, we often convert them to g_i(x) &lt;= 0.\n\nFeasible Set (S): This is the collection of all design points (vectors x) that satisfy all the constraints. A design is feasible if it belongs to this set.\n**Optimum Solution (x*): This is the design variable vector x** that yields the best value of the objective function (minimum or maximum) while satisfying all constraints. The corresponding objective function value is f(x*).\nGradient Vector (‚àáf(x)): For a function f(x) of multiple variables, the gradient vector is a vector of its partial derivatives with respect to each variable. It points in the direction of the steepest increase of the function.\nHessian Matrix (H(x)): This is a square matrix of second-order partial derivatives of a function. It is important for determining the nature of an optimum point (e.g., local minimum, maximum, or saddle point).\nContinuous Functions/Variables: Variables that can take any real value within a range, and functions whose graphs can be drawn without lifting the pen.\nDifferentiable Functions: Functions for which derivatives can be calculated. Note that a function can be continuous everywhere but not differentiable everywhere (e.g., f(x) = |x| is continuous at x = 0 but not differentiable there).\n\nWe will define these terms in more detail and review relevant mathematical operations as we encounter them in the context of solving optimization problems. It‚Äôs important to understand and internalize this terminology as it forms the language of optimum design.\nThis concludes our introduction to Design Optimization. In the next lecture, we will dive into the critical step of formulating an optimum design problem mathematically.",
    "crumbs": [
      "üìñ Lectures",
      "Lecture 1: Introduction"
    ]
  },
  {
    "objectID": "documents/05-Lecture_03.html",
    "href": "documents/05-Lecture_03.html",
    "title": "Lecture 3: Graphical Solution Method and Basic Optimization Concepts",
    "section": "",
    "text": "Lecture 3: Graphical Solution Method and Basic Optimization Concepts\nIn our previous lectures, we defined design optimization and learned how to mathematically formulate an optimum design problem by identifying design variables, objective functions, and constraints. Today, we‚Äôre going to explore a powerful visual tool for understanding these concepts: the Graphical Solution Method. This method is particularly insightful because it allows us to visualize the design space and the optimization process, albeit primarily for problems with a limited number of design variables.\nWhile the graphical method is generally applicable only to problems with two design variables, it is invaluable for building an intuitive understanding of core optimization ideas such as the feasible region, objective function contours, and the nature of an optimum solution. These concepts and their associated terminology are foundational and will be used throughout the rest of this course when we discuss more advanced numerical methods.\n\nThe Graphical Solution Process\nThe essence of the graphical method is to represent the design variables as coordinates on a 2D plane and then plot the objective function and constraints on this plane. This allows us to visually identify the ‚Äúbest‚Äù design.\nHere‚Äôs a step-by-step procedure:\n\nStep 1: Define the Design Space and Plot Design Variables\nThe first step is to establish a coordinate system for your two design variables, say \\(x_1\\) and \\(x_2\\). Each point on this plane represents a unique design. Since design variables are typically physical quantities, they are usually non-negative, meaning we focus on the first quadrant of the coordinate system.\n\n\nStep 2: Plot the Constraints and Identify the Feasible Region\nThis is a critical step. Each constraint (whether equality or inequality) will define a boundary or a region on your design plane.\n\nPlotting Inequality Constraints: For each inequality constraint, \\(g_j(\\mathbf{x}) \\le 0\\) (or \\(g_j(\\mathbf{x}) \\ge 0\\)), first treat it as an equality (\\(g_j(\\mathbf{x}) = 0\\)) to draw its boundary line or curve. Once the boundary is drawn, you need to determine which side of the boundary satisfies the inequality. A simple way to do this is to pick a test point (e.g., the origin (0,0), if it doesn‚Äôt lie on the boundary) and substitute its coordinates into the inequality. If the inequality is satisfied, that side is feasible; otherwise, the other side is feasible. You can shade the infeasible region or draw arrows pointing towards the feasible region.\nPlotting Equality Constraints: For equality constraints, \\(h_k(\\mathbf{x}) = 0\\), the feasible region lies exactly on the line or curve defined by the equation. This can significantly restrict the feasible design space.\nSide Constraints (Bounds): These define simple rectangular regions (e.g., \\(x_{1L} \\le x_1 \\le x_{1U}\\)). Plotting these creates vertical and horizontal lines that further restrict the design space.\n\nThe feasible region (or feasible set) is the area on the design plane where all constraints are simultaneously satisfied. It is the intersection of all feasible sides of the inequalities and the exact lines of the equalities. If no such region exists, the problem is infeasible.\n\n\nStep 3: Plot the Objective Function Contours\nThe objective function, \\(f(\\mathbf{x})\\), needs to be visualized. To do this, we plot contours (also known as iso-cost or iso-profit lines). A contour represents all design points where the objective function has a constant value. For a minimization problem, you draw lines or curves where \\(f(\\mathbf{x}) = C_1, f(\\mathbf{x}) = C_2, \\ldots\\), where \\(C_1 &lt; C_2 &lt; \\ldots\\). For a maximization problem, \\(C_1 &gt; C_2 &gt; \\ldots\\). By observing the direction in which the objective function value improves (decreases for minimization, increases for maximization), you can determine the ‚Äúdescent‚Äù or ‚Äúascent‚Äù direction.\n\n\nStep 4: Identify the Optimum Solution\nThe optimum solution is the point within the feasible region that yields the best (minimum or maximum) value for the objective function. * For minimization, slide the objective function contours in the direction of decreasing values until the contour just touches the feasible region. The last point(s) of contact are the optimum. * For maximization, slide the objective function contours in the direction of increasing values until the contour just touches the feasible region. The last point(s) of contact are the optimum.\nThe optimum solution typically occurs at: * A vertex (corner point) of the feasible region. * Along an active constraint boundary if the objective function is parallel to that boundary (resulting in multiple optimum solutions). * At a point on a curved constraint boundary if the objective function is tangent to it.\nAn active constraint at the optimum point is a constraint that is satisfied as an equality (i.e., \\(g_j(\\mathbf{x}^*) = 0\\) or \\(h_k(\\mathbf{x}^*) = 0\\)). Constraints for which \\(g_j(\\mathbf{x}^*) &lt; 0\\) are inactive.\n\n\n\nSolved Examples\nLet‚Äôs illustrate this with the problems we formulated in Lecture 2.\n\nExample 1: Maximize Volume of a Coffee Mug\nRecall the problem formulation from Lecture 2:\nMaximize \\(f(x_1, x_2) = \\pi x_1^2 x_2\\) (where \\(x_1\\) = radius \\(R\\), \\(x_2\\) = height \\(H\\))\nSubject to:\n\n\\(g_1(x_1, x_2) = x_2 - 20 \\le 0 \\quad \\Rightarrow x_2 \\le 20\\)\n\\(g_2(x_1, x_2) = x_1 - 20 \\le 0 \\quad \\Rightarrow x_1 \\le 20\\)\n\\(g_3(x_1, x_2) = 5 - x_1 \\le 0 \\quad \\Rightarrow x_1 \\ge 5\\)\n\\(g_4(x_1, x_2) = 2\\pi x_1 x_2 - 900 \\le 0 \\quad \\Rightarrow 2\\pi x_1 x_2 \\le 900\\)\n\\(x_1 \\ge 0\\)\n\\(x_2 \\ge 0\\)\n\nLet‚Äôs set up the graphical solution:\n\nDesign Space: We‚Äôll use \\(x_1\\) (radius) on the horizontal axis and \\(x_2\\) (height) on the vertical axis. Since \\(x_1 \\ge 5\\) and \\(x_2 \\ge 0\\), we are in the positive quadrant, specifically starting from \\(x_1=5\\).\nPlot Constraints and Feasible Region:\n\n\\(g_1: x_2 \\le 20\\). This is a horizontal line at \\(x_2 = 20\\). The feasible region is below this line.\n\\(g_2: x_1 \\le 20\\). This is a vertical line at \\(x_1 = 20\\). The feasible region is to the left of this line.\n\\(g_3: x_1 \\ge 5\\). This is a vertical line at \\(x_1 = 5\\). The feasible region is to the right of this line.\n\\(g_4: 2\\pi x_1 x_2 \\le 900 \\Rightarrow x_1 x_2 \\le \\frac{900}{2\\pi} \\approx 143.2\\). This is a hyperbolic curve. The feasible region is below this curve.\n\nThe feasible region is bounded by \\(x_1=5\\), \\(x_1=20\\), \\(x_2=0\\), \\(x_2=20\\), and the curve \\(x_1 x_2 = 143.2\\).\nPlot Objective Function Contours: Objective function: \\(f(x_1, x_2) = \\pi x_1^2 x_2\\). To maximize this, we‚Äôll draw contours of \\(f(x_1, x_2) = C\\). For example, let‚Äôs pick some values for \\(C\\):\n\nIf \\(C = 5000\\), \\(\\pi x_1^2 x_2 = 5000 \\Rightarrow x_1^2 x_2 \\approx 1591.5\\)\nIf \\(C = 10000\\), \\(\\pi x_1^2 x_2 = 10000 \\Rightarrow x_1^2 x_2 \\approx 3183.1\\)\nIf \\(C = 15000\\), \\(\\pi x_1^2 x_2 = 15000 \\Rightarrow x_1^2 x_2 \\approx 4774.6\\)\n\nThese contours are curves that move away from the origin as \\(C\\) increases.\nIdentify the Optimum Solution: Visually ‚Äúpush‚Äù the objective function contours towards higher values (to maximize) within the feasible region. The last point where a contour touches the feasible region will be the optimum.\nIn this problem, the objective function \\(\\pi x_1^2 x_2\\) grows faster with \\(x_1\\) than \\(x_2\\). We expect the optimum to be where \\(x_1\\) is as large as possible, but restricted by constraints. The most restrictive boundary for large \\(x_1\\) and \\(x_2\\) will likely be \\(x_2=20\\) and \\(x_1 x_2 \\approx 143.2\\). Let‚Äôs check the intersection of \\(x_2 = 20\\) and \\(x_1 x_2 = 143.2\\): \\(x_1 (20) = 143.2 \\Rightarrow x_1 = \\frac{143.2}{20} = 7.16\\). At this point, \\(x_1 = 7.16\\) and \\(x_2 = 20\\). This point is within all bounds (\\(5 \\le 7.16 \\le 20\\), \\(0 \\le 20 \\le 20\\)). The volume at this point: \\(f(7.16, 20) = \\pi (7.16)^2 (20) \\approx 3217\\) cm\\(^3\\).\nConsider if \\(x_1=20\\) is the limiting factor. If \\(x_1=20\\), then \\(2\\pi (20) x_2 \\le 900 \\Rightarrow 40\\pi x_2 \\le 900 \\Rightarrow x_2 \\le \\frac{900}{40\\pi} \\approx 7.16\\). So, another potential optimum could be at \\(x_1=20, x_2=7.16\\). The volume here: \\(f(20, 7.16) = \\pi (20)^2 (7.16) \\approx 9000\\) cm\\(^3\\). This is a much higher value.\nThe optimum will lie on the boundary of the feasible region defined by \\(x_1=20\\) and \\(2\\pi x_1 x_2 = 900\\). Specifically, the highest value will be found by pushing the objective function \\(f(x_1, x_2) = \\pi x_1^2 x_2\\) as far as possible towards the upper-right corner. This typically leads to a point on the intersection of active constraints.\nThe optimum occurs at the intersection of the constraints \\(x_1 = 20\\) and \\(2\\pi x_1 x_2 = 900\\). Substitute \\(x_1 = 20\\) into \\(2\\pi x_1 x_2 = 900\\): \\(2\\pi (20) x_2 = 900 \\Rightarrow 40\\pi x_2 = 900 \\Rightarrow x_2 = \\frac{900}{40\\pi} \\approx 7.16\\). So, the optimum design is approximately \\(x_1^* = 20\\) cm and \\(x_2^* = 7.16\\) cm. The maximum volume is \\(f(20, 7.16) = \\pi (20)^2 (7.16) \\approx 9000\\) cm\\(^3\\).\nAt this point, the active constraints are \\(g_2 (x_1 - 20 = 0)\\) and \\(g_4 (2\\pi x_1 x_2 - 900 = 0)\\).\n\n\n\n\n\n\n\nFigure¬†1: maximum volume mug\n\n\n\n\nExample 2: Minimum-Weight Tubular Column Design (Graphical Solution)\nLet‚Äôs use the minimum-weight tubular column problem formulated in Section 2.7, with specific data as given in a similar problem from the source.\nGiven Data: * \\(P = 100 \\text{ kN}\\) (Applied load) * \\(L = 5.0 \\text{ m}\\) (Column length) * \\(E = 210 \\text{ GPa}\\) (Modulus of elasticity) * \\(\\sigma_a = 250 \\text{ MPa}\\) (Allowable stress) * \\(\\rho = 7850 \\text{ kg/m}^3\\) (Mass density)\nFormulation 1 from Lecture 2 (with \\(R = x_1\\), \\(t = x_2\\)):\nMinimize \\(f(x_1, x_2) = 2\\pi \\rho L x_1 x_2\\) Substituting numerical values for \\(\\rho\\) and \\(L\\): \\(f(x_1, x_2) = 2\\pi (7850)(5.0) x_1 x_2 = 246602.8 x_1 x_2\\) (in kg)\nSubject to:\n\nStress Constraint:\n\\(\\frac{P}{2\\pi x_1 x_2 \\sigma_a} - 1 \\le 0\\)\n\\(\\frac{100 \\times 10^3 \\text{ N}}{2\\pi x_1 x_2 (250 \\times 10^6 \\text{ N/m}^2)} - 1 \\le 0\\) \\(\\frac{100 \\times 10^3}{500\\pi \\times 10^6 x_1 x_2} - 1 \\le 0 \\Rightarrow \\frac{1}{\\pi \\times 5000 x_1 x_2} - 1 \\le 0 \\Rightarrow \\frac{1}{15707963 x_1 x_2} - 1 \\le 0\\) \\(g_1(x_1, x_2) = \\frac{0.0000000636}{x_1 x_2} - 1 \\le 0 \\Rightarrow x_1 x_2 \\ge 0.0000000636 \\text{ m}^2\\)\nBuckling Constraint:\n\\(\\frac{4 P L^2}{\\pi^3 E x_1^3 x_2} - 1 \\le 0\\) \\(\\frac{4 (100 \\times 10^3)(5.0)^2}{\\pi^3 (210 \\times 10^9) x_1^3 x_2} - 1 \\le 0\\) \\(\\frac{10000 \\times 10^3}{\\pi^3 (210 \\times 10^9) x_1^3 x_2} - 1 \\le 0 \\Rightarrow \\frac{10 \\times 10^6}{6550 \\times 10^9 x_1^3 x_2} - 1 \\le 0\\) \\(g_2(x_1, x_2) = \\frac{0.0000015267}{x_1^3 x_2} - 1 \\le 0 \\Rightarrow x_1^3 x_2 \\ge 0.0000015267 \\text{ m}^4\\)\nManufacturing/Geometric Constraint:\n\\(\\frac{x_1}{x_2} - 50 \\le 0 \\Rightarrow x_1 \\le 50 x_2\\)\nSide Constraints (Bounds on Design Variables): As specified in the problem in the sources: \\(0 \\le x_1 \\le 0.4 \\text{ m}\\) (mean radius) \\(0 \\le x_2 \\le 0.1 \\text{ m}\\) (wall thickness)\n\nNow, let‚Äôs proceed with the graphical solution:\n\nDesign Space: Plot \\(x_1\\) (mean radius) on the horizontal axis and \\(x_2\\) (wall thickness) on the vertical axis. We are interested in the region \\(x_1 \\ge 0, x_2 \\ge 0\\). The bounds \\(x_1 \\le 0.4\\) and \\(x_2 \\le 0.1\\) define a rectangular region.\nPlot Constraints and Feasible Region:\n\n\\(x_1 \\ge 0, x_2 \\ge 0\\): This is the first quadrant.\n\\(x_1 \\le 0.4\\): A vertical line at \\(x_1 = 0.4\\). Feasible to the left.\n\\(x_2 \\le 0.1\\): A horizontal line at \\(x_2 = 0.1\\). Feasible below.\n\\(g_1: x_1 x_2 \\ge 0.0000000636\\): This is a hyperbola. The feasible region is above this curve. (e.g., if \\(x_1 = 0.1 \\text{ m}\\), \\(x_2 \\ge 0.000000636 \\text{ m}\\))\n\\(g_2: x_1^3 x_2 \\ge 0.0000015267\\): This is also a curve. The feasible region is above this curve. (e.g., if \\(x_1 = 0.1 \\text{ m}\\), \\(x_2 \\ge 0.0000015267 / (0.1)^3 = 0.0015267 \\text{ m}\\))\n\\(g_3: x_1 \\le 50 x_2 \\Rightarrow x_2 \\ge \\frac{x_1}{50}\\): This is a straight line through the origin with a slope of \\(1/50 = 0.02\\). The feasible region is above this line.\n\nThe feasible region is the area where all these conditions overlap. It will be bounded by \\(x_1=0.4\\), \\(x_2=0.1\\), and the curves from \\(g_1, g_2, g_3\\).\nPlot Objective Function Contours: Objective function: \\(f(x_1, x_2) = 246602.8 x_1 x_2\\). We want to minimize this. Draw contours \\(x_1 x_2 = C'\\). As \\(C'\\) decreases, the hyperbolas move closer to the origin.\nIdentify the Optimum Solution: We want to find the smallest value of \\(C'\\) such that the hyperbola \\(x_1 x_2 = C'\\) still touches the feasible region. This will occur at a point where an objective function contour is tangent to one or more active constraints.\nLet‚Äôs analyze the constraints. The objective is to minimize \\(x_1 x_2\\). The region is generally pushed towards the lower-left corner by the objective function. The constraints \\(g_1\\) and \\(g_2\\) push the region away from the origin. The most likely optimum will be at the intersection of \\(g_1\\) and \\(g_2\\), as they define the lower-left boundary of the feasible region, where the mass function \\(x_1 x_2\\) would naturally be smallest.\nLet‚Äôs find the intersection of \\(g_1\\) and \\(g_2\\): From \\(g_1\\): \\(x_2 = \\frac{0.0000000636}{x_1}\\) Substitute this into \\(g_2\\): \\(x_1^3 \\left(\\frac{0.0000000636}{x_1}\\right) = 0.0000015267\\) \\(0.0000000636 x_1^2 = 0.0000015267\\) \\(x_1^2 = \\frac{0.0000015267}{0.0000000636} \\approx 24.0047\\) \\(x_1 = \\sqrt{24.0047} \\approx 4.899 \\text{ m}\\)\nThis value of \\(x_1\\) is much larger than the upper bound of \\(0.4 \\text{ m}\\). This indicates that the buckling and stress constraints (if they are the only active ones) would lead to a very large column. This suggests that the side constraints (\\(x_1 \\le 0.4\\), \\(x_2 \\le 0.1\\)) are likely to be active.\nLet‚Äôs re-evaluate. The optimum solution for this problem, as given in a similar exercise in the source, is often at a point where the constraints \\(g_2\\) and \\(g_3\\) are active, or \\(g_1\\) and \\(g_2\\). However, due to the tight bounds, we should check combinations with the bounds.\nThe example solution in source provides \\(R^* = 0.046 \\text{ m}\\) and \\(t^* = 0.0022 \\text{ m}\\) with \\(f^* = 0.0229 \\text{ kg}\\). Let‚Äôs verify these by plugging into our numerical constraint forms: \\(x_1 = 0.046 \\text{ m}\\), \\(x_2 = 0.0022 \\text{ m}\\).\n\nObjective: \\(f(0.046, 0.0022) = 246602.8 \\times 0.046 \\times 0.0022 \\approx 25.0 \\text{ kg}\\). (This differs from the source‚Äôs \\(f^*\\) which might be for slightly different units or a different calculation, but the values for R and t from the source are good to check for feasibility). Let‚Äôs use the given data for \\(P, E, \\rho, L, \\sigma_a\\) and re-calculate mass: \\(f(x_1, x_2) = 2\\pi (7833)(5.0) x_1 x_2 = 246080.0 x_1 x_2\\). So, \\(f(0.046, 0.0022) = 246080.0 \\times 0.046 \\times 0.0022 \\approx 24.93 \\text{ kg}\\). Still not 0.0229 kg, but this just means the given \\(f^*\\) is for slightly different values. The constraints are what matters for finding the optimal point.\n\\(x_1 \\le 0.4\\): \\(0.046 \\le 0.4\\) (satisfied)\n\\(x_2 \\le 0.1\\): \\(0.0022 \\le 0.1\\) (satisfied)\n\\(g_1: x_1 x_2 \\ge 0.0000000636 \\Rightarrow (0.046)(0.0022) = 0.0001012 \\ge 0.0000000636\\) (satisfied, inactive)\n\\(g_2: x_1^3 x_2 \\ge 0.0000015267 \\Rightarrow (0.046)^3 (0.0022) = (0.000097336)(0.0022) = 0.000000214 \\ge 0.0000015267\\) (This is not satisfied. \\(0.000000214\\) is less than \\(0.0000015267\\), so the buckling constraint is violated if these are the values. The values from the source are for a different data set P=10 MN, E=207 GPa, L=5.0m, sigma_a=248 MPa. So my numerical values are different from the source. I should use the specific problem in Exercise 3.33 of the source for values.)\n\nLet‚Äôs re-run with the data from Exercise 3.33: \\(P = 100 \\text{ kN}\\) \\(l = 5.0 \\text{ m}\\) \\(E = 210 \\text{ GPa}\\) \\(\\sigma_a = 250 \\text{ MPa}\\) \\(\\rho = 7850 \\text{ kg/m}^3\\) \\(R \\le 0.4 \\text{ m}\\), \\(t \\le 0.1 \\text{ m}\\), \\(R, t \\ge 0\\)\nRecalculate Constraints with these parameters:\n\n\\(g_1(x_1, x_2) = \\frac{P}{2\\pi x_1 x_2 \\sigma_a} - 1 \\le 0 \\Rightarrow \\frac{100 \\times 10^3}{2\\pi x_1 x_2 (250 \\times 10^6)} - 1 \\le 0 \\Rightarrow \\frac{6.366 \\times 10^{-5}}{x_1 x_2} - 1 \\le 0 \\Rightarrow x_1 x_2 \\ge 6.366 \\times 10^{-5}\\)\n\\(g_2(x_1, x_2) = \\frac{4 P L^2}{\\pi^3 E x_1^3 x_2} - 1 \\le 0 \\Rightarrow \\frac{4 (100 \\times 10^3) (5.0)^2}{\\pi^3 (210 \\times 10^9) x_1^3 x_2} - 1 \\le 0 \\Rightarrow \\frac{1.526 \\times 10^{-6}}{x_1^3 x_2} - 1 \\le 0 \\Rightarrow x_1^3 x_2 \\ge 1.526 \\times 10^{-6}\\)\n\\(g_3(x_1, x_2) = \\frac{x_1}{x_2} - 50 \\le 0 \\Rightarrow x_1 \\le 50 x_2 \\Rightarrow x_2 \\ge \\frac{x_1}{50}\\) (Assuming \\(R/t \\le 50\\) is for thin-walled tube. If \\(R \\gg t\\), then this constraint ensures it is thin. If we use this as an upper bound on ratio \\(R/t\\), it‚Äôs for manufacturability or avoiding local buckling, as stated in Lecture 2. If it‚Äôs \\(R/t \\ge 10\\), that‚Äôs for thin-walled assumption. Let‚Äôs assume it means \\(R \\le 50t\\).)\n\nGraphical Solution Analysis (Conceptual): The objective is to minimize \\(f(x_1, x_2) = 246602.8 x_1 x_2\\). The contours are hyperbolas that decrease in value as they approach the origin.\n\n\\(x_1 \\ge 0, x_2 \\ge 0\\) (first quadrant)\n\\(x_1 \\le 0.4\\) (vertical line)\n\\(x_2 \\le 0.1\\) (horizontal line)\n\\(g_1: x_1 x_2 \\ge 6.366 \\times 10^{-5}\\) (hyperbola, feasible above)\n\\(g_2: x_1^3 x_2 \\ge 1.526 \\times 10^{-6}\\) (steeper curve, feasible above)\n\\(g_3: x_2 \\ge x_1/50\\) (line through origin, feasible above)\n\nThe feasible region will be bounded by the outer limits of these constraints and the bounds \\(x_1=0.4, x_2=0.1\\). To minimize \\(x_1 x_2\\), we need to find the point in the feasible region closest to the origin. This point will be on one of the ‚Äúlower‚Äù boundaries of the feasible region, meaning where some constraints are active.\nLet‚Äôs find the intersection points:\n\nIntersection of \\(g_1\\) and \\(g_2\\): From \\(g_1\\): \\(x_2 = \\frac{6.366 \\times 10^{-5}}{x_1}\\) Substitute into \\(g_2\\): \\(x_1^3 \\left(\\frac{6.366 \\times 10^{-5}}{x_1}\\right) = 1.526 \\times 10^{-6}\\) \\(6.366 \\times 10^{-5} x_1^2 = 1.526 \\times 10^{-6}\\) \\(x_1^2 = \\frac{1.526 \\times 10^{-6}}{6.366 \\times 10^{-5}} \\approx 0.02397\\) \\(x_1 = \\sqrt{0.02397} \\approx 0.1548 \\text{ m}\\) Then, \\(x_2 = \\frac{6.366 \\times 10^{-5}}{0.1548} \\approx 0.000411 \\text{ m}\\)\nLet‚Äôs check if this point \\((0.1548, 0.000411)\\) is feasible with respect to other constraints:\n\n\\(x_1 \\le 0.4\\): \\(0.1548 \\le 0.4\\) (satisfied)\n\\(x_2 \\le 0.1\\): \\(0.000411 \\le 0.1\\) (satisfied)\n\\(g_3: x_2 \\ge x_1/50 \\Rightarrow 0.000411 \\ge 0.1548/50 \\Rightarrow 0.000411 \\ge 0.003096\\) (NOT satisfied! \\(g_3\\) is violated). This means the optimum is not at the intersection of \\(g_1\\) and \\(g_2\\), but rather on the boundary defined by \\(g_3\\).\n\nIntersection of \\(g_2\\) and \\(g_3\\): From \\(g_3\\): \\(x_2 = x_1/50\\) Substitute into \\(g_2\\): \\(x_1^3 (x_1/50) = 1.526 \\times 10^{-6}\\) \\(x_1^4 / 50 = 1.526 \\times 10^{-6}\\) \\(x_1^4 = 50 \\times 1.526 \\times 10^{-6} = 7.63 \\times 10^{-5}\\) \\(x_1 = (7.63 \\times 10^{-5})^{1/4} \\approx 0.0934 \\text{ m}\\) Then, \\(x_2 = x_1/50 = 0.0934/50 = 0.001868 \\text{ m}\\)\nLet‚Äôs check if this point \\((0.0934, 0.001868)\\) is feasible with respect to other constraints:\n\n\\(x_1 \\le 0.4\\): \\(0.0934 \\le 0.4\\) (satisfied)\n\\(x_2 \\le 0.1\\): \\(0.001868 \\le 0.1\\) (satisfied)\n\\(g_1: x_1 x_2 \\ge 6.366 \\times 10^{-5} \\Rightarrow (0.0934)(0.001868) = 0.000174 \\ge 6.366 \\times 10^{-5}\\) (satisfied, inactive)\n\nSince this point satisfies all constraints, this is a candidate optimum point. At this point, \\(x_1^* = 0.0934 \\text{ m}\\) and \\(x_2^* = 0.001868 \\text{ m}\\). The minimum mass would be \\(f(0.0934, 0.001868) = 246602.8 \\times 0.0934 \\times 0.001868 \\approx 42.9 \\text{ kg}\\).\n\nThis graphical approach visually confirms that the optimum solution for two-variable problems often lies at the intersection of active constraints, particularly on the boundary of the feasible region that is ‚Äúclosest‚Äù to the direction of optimization for the objective function.\n\n\n\n\n\n\n\nFigure¬†2: maximum volume mug\n\n\n\n\n\n\n\n\n\nThe optimization formulation for Problem 3.22, which involves a cantilever hollow circular beam-column from Exercise 2.23, can be developed by following a five-step procedure: project/problem description, data and information collection, definition of design variables, optimization criterion, and formulation of constraints. The problem aims to translate a descriptive design problem into a well-defined mathematical statement for optimization.\n\n\nThe problem is to solve graphically the cantilever beam problem of Exercise 2.23 [U1]. Exercise 2.23 describes the design of a hollow circular beam-column with outer radius (Ro) and inner radius (Ri). The goal is to minimize mass while satisfying stress and dimension constraints.\n\n\n{#fig-cantilever-beam)\n\n\nSpecified Data (from Problem 3.22 and Exercise 2.23) [U1, 84]: * Applied load, P = 10 kN = 10,000 N * Length of the beam, L = 5.0 m = 5,000 mm * Modulus of elasticity, E = 210 GPa = 210,000 N/mm¬≤ (Note: E is not explicitly used in the stress formulas given for Exercise 2.23, which focuses on bending and shear stresses.) * Allowable bending stress, œÉa = 250 MPa = 250 N/mm¬≤ * Allowable shear stress, œÑa = 90 MPa = 90 N/mm¬≤ * Mass density, œÅ = 7850 kg/m¬≥ = 7.85 √ó 10‚Åª‚Å∂ kg/mm¬≥ (since 1 m¬≥ = 10‚Åπ mm¬≥)\n\n\n1. Definition of Design Variables: The design variables, which are the quantities the designer can change, are the outer and inner radii of the hollow circular cross-section. * Ro (Outer radius, in mm) * Ri (Inner radius, in mm)\n\n\n2. Optimization Criterion (Objective Function): For structural components, minimizing mass or weight is a common objective. The total mass of the column is given by Mass = œÅ * L * A, where A is the cross-sectional area. The cross-sectional area for a hollow circular section is A = œÄ * (Ro¬≤ - Ri¬≤).\n\n\nTherefore, the objective function to be minimized is: Minimize f(Ro, Ri) = œÅ * L * œÄ * (Ro¬≤ - Ri¬≤) Substituting the given data:\n\n\nf(Ro, Ri) = (7.85 √ó 10‚Åª‚Å∂ kg/mm¬≥) * (5000 mm) * œÄ * (Ro¬≤ - Ri¬≤) mm¬≤\n\n\nf(Ro, Ri) = 0.03925 * œÄ * (Ro¬≤ - Ri¬≤) kg\n\n\n3. Formulation of Constraints: Constraints naturally arise in optimum design problems to ensure the system meets performance requirements and physical limitations. All constraints must be functions of at least one design variable. For numerical optimization, ‚Äúgreater than type‚Äù constraints are typically converted to ‚Äúless than type‚Äù (‚â§ 0) by multiplying by -1.\n\n\nThe constraints for this problem are: * Upper bound on outer radius [U1]: Ro ‚â§ 20.0 cm = 200 mm g1(Ro, Ri) = Ro - 200 ‚â§ 0 * Upper bound on inner radius [U1]: Ri ‚â§ 20.0 cm = 200 mm g2(Ro, Ri) = Ri - 200 ‚â§ 0 * Inner radius must be less than outer radius: For a hollow section, the inner radius must be strictly smaller than the outer radius (Ri &lt; Ro). g3(Ro, Ri) = Ri - Ro ‚â§ 0 (Note: For numerical stability, some implementations might use Ri - Ro ‚â§ -Œµ where Œµ is a small positive number, or ensure Ro^4 - Ri^4 is not zero in other ways, as Ro^4 - Ri^4 appears in the denominator of stress calculations). * Non-negativity of radii: Radii must be positive. g4(Ro, Ri) = -Ro ‚â§ 0 g5(Ro, Ri) = -Ri ‚â§ 0 * Allowable bending stress: The maximum bending stress (œÉ) must not exceed the allowable bending stress (œÉa). œÉ = (P * L * Ro) / I, where I = (œÄ/4) * (Ro‚Å¥ - Ri‚Å¥) is the moment of inertia. g6(Ro, Ri) = (P * L * Ro) / ((œÄ/4) * (Ro‚Å¥ - Ri‚Å¥)) - œÉa ‚â§ 0 Substituting data: g6(Ro, Ri) = (10000 N * 5000 mm * Ro) / ((œÄ/4) * (Ro‚Å¥ - Ri‚Å¥)) - 250 N/mm¬≤ ‚â§ 0 g6(Ro, Ri) = (50,000,000 * Ro) / (0.785398 * (Ro‚Å¥ - Ri‚Å¥)) - 250 ‚â§ 0 * Allowable shear stress: The maximum shearing stress (œÑ) must not exceed the allowable shear stress (œÑa). œÑ = (P / I) * ((Ro¬≥ + Ri¬≥) / (Ro¬≤ + Ri¬≤)). g7(Ro, Ri) = (P * ((Ro¬≥ + Ri¬≥) / (Ro¬≤ + Ri¬≤))) / ((œÄ/4) * (Ro‚Å¥ - Ri‚Å¥)) - œÑa ‚â§ 0 Substituting data: g7(Ro, Ri) = (10000 N * ((Ro¬≥ + Ri¬≥) / (Ro¬≤ + Ri¬≤))) / ((œÄ/4) * (Ro‚Å¥ - Ri‚Å¥)) - 90 N/mm¬≤ ‚â§ 0 g7(Ro, Ri) = (10000 * ((Ro¬≥ + Ri¬≥) / (Ro¬≤ + Ri¬≤))) / (0.785398 * (Ro‚Å¥ - Ri‚Å¥)) - 90 ‚â§ 0\n\n\nOptimization Formulation: Find the design variables Ro and Ri (in mm).\n\n\nTo Minimize: f(Ro, Ri) = 0.03925 * œÄ * (Ro¬≤ - Ri¬≤) (kg)\n\n\nSubject to the following inequality constraints: 1. g1(Ro, Ri) = Ro - 200 ‚â§ 0 2. g2(Ro, Ri) = Ri - 200 ‚â§ 0 3. g3(Ro, Ri) = Ri - Ro ‚â§ 0 4. g4(Ro, Ri) = -Ro ‚â§ 0 5. g5(Ro, Ri) = -Ri ‚â§ 0 6. g6(Ro, Ri) = (50,000,000 * Ro) / (0.785398 * (Ro‚Å¥ - Ri‚Å¥)) - 250 ‚â§ 0 7. g7(Ro, Ri) = (10000 * ((Ro¬≥ + Ri¬≥) / (Ro¬≤ + Ri¬≤))) / (0.785398 * (Ro‚Å¥ - Ri‚Å¥)) - 90 ‚â§ 0\n\n\nThis problem is a continuous-variable, nonlinear programming (NLP) problem, as the design variables can take any numerical value within their allowed range, and the objective and constraint functions are nonlinear. Due to having only two design variables, it can be solved graphically.\n\n\n\nThis concludes our exploration of the Graphical Solution Method. It provides a robust foundation for understanding the interaction between objective functions and constraints, and for identifying feasible regions and optimum points. In the next lecture, we will move on to more analytical methods for optimum design, starting with Linear Programming.",
    "crumbs": [
      "üìñ Lectures",
      "Lecture 3: Graphical Solution Method and Basic Optimization Concepts"
    ]
  },
  {
    "objectID": "documents/07-Lecture_05.html",
    "href": "documents/07-Lecture_05.html",
    "title": "Lecture 5: Lagrangian Methods for Optimum Design",
    "section": "",
    "text": "Lecture 5: Lagrangian Methods for Optimum Design\nIn our journey through Design Optimization, we‚Äôve progressed from graphically understanding basic optimization concepts to systematically solving linear problems using Linear Programming. Today, we confront the more general and pervasive class of problems in engineering: Nonlinear Programming (NLP). Unlike LP, where everything is linear, NLP problems involve objective functions or constraints (or both) that are nonlinear. For these complex problems, we often cannot rely on simple graphical methods or direct algebraic solutions beyond very small cases.\nThis lecture introduces a fundamental analytical technique for solving constrained nonlinear optimization problems: Lagrangian Methods. The cornerstone of this approach is the Karush‚ÄìKuhn‚ÄìTucker (KKT) necessary conditions, which provide a set of equations that must be satisfied at any local optimum point of a constrained optimization problem.\nBy the end of this lecture, you will be able to define the Lagrangian function, state and interpret the Karush‚ÄìKuhn‚ÄìTucker (KKT) necessary conditions, and apply these conditions to find candidate optimum solutions for constrained nonlinear design problems.\n\nGeneral Nonlinear Programming Problem (NLP)\nRecall the general mathematical model for an optimum design problem we formulated in Lecture 2:\nFind the design variable vector \\(\\mathbf{x} = (x_1, x_2, ..., x_n)\\)\nTo minimize (or maximize) the objective function \\(f(\\mathbf{x})\\)\nSubject to: * Equality constraints: \\(h_k(\\mathbf{x}) = 0\\), for \\(k = 1, ..., p\\) * Inequality constraints: \\(g_j(\\mathbf{x}) \\le 0\\), for \\(j = 1, ..., m\\) * Side constraints (bounds): \\(x_{iL} \\le x_i \\le x_{iU}\\), for \\(i = 1, ..., n\\)\nFor Lagrangian methods, we often treat the side constraints as general inequality constraints. So, the problem can be viewed as: Minimize \\(f(\\mathbf{x})\\) subject to \\(h_k(\\mathbf{x}) = 0\\) and \\(g_j(\\mathbf{x}) \\le 0\\). The functions \\(f(\\mathbf{x})\\), \\(h_k(\\mathbf{x})\\), and \\(g_j(\\mathbf{x})\\) can be nonlinear.\n\n\nThe Lagrangian Function\nThe central idea behind Lagrangian methods is to transform a constrained optimization problem into an unconstrained one by introducing a new function called the Lagrangian function, \\(L\\). This function incorporates all the constraints into the objective function using special coefficients called Lagrange multipliers.\nThe Lagrangian function for a general NLP problem is defined as: \\(L(\\mathbf{x}, \\mathbf{v}, \\mathbf{u}) = f(\\mathbf{x}) + \\sum_{k=1}^{p} v_k h_k(\\mathbf{x}) + \\sum_{j=1}^{m} u_j g_j(\\mathbf{x})\\)\nWhere: * \\(f(\\mathbf{x})\\) is the objective function. * \\(h_k(\\mathbf{x})\\) are the equality constraints. * \\(g_j(\\mathbf{x})\\) are the inequality constraints. * \\(v_k\\) are the Lagrange multipliers associated with the equality constraints \\(h_k(\\mathbf{x})\\). These are unrestricted in sign. * \\(u_j\\) are the Lagrange multipliers associated with the inequality constraints \\(g_j(\\mathbf{x})\\). These must be non-negative.\nThe Lagrange multipliers \\(v_k\\) and \\(u_j\\) are additional variables introduced into the problem. We are now looking for an optimum point \\((\\mathbf{x}^*, \\mathbf{v}^*, \\mathbf{u}^*)\\) that satisfies certain conditions.\n\n\nKarush‚ÄìKuhn‚ÄìTucker (KKT) Necessary Conditions\nFor a given NLP problem, if a design point \\(\\mathbf{x}^*\\) is a local minimum, and certain regularity conditions (like constraint qualifications, which are typically satisfied for well-behaved engineering problems) hold, then there must exist Lagrange multipliers \\(\\mathbf{v}^*\\) and \\(\\mathbf{u}^*\\) such that the Karush‚ÄìKuhn‚ÄìTucker (KKT) necessary conditions are satisfied. These conditions essentially generalize the concept of finding where the gradient is zero for unconstrained optimization to problems with constraints.\nThe KKT conditions are:\n\nGradient of the Lagrangian with respect to design variables must be zero: \\(\\frac{\\partial L}{\\partial x_i}(\\mathbf{x}^*, \\mathbf{v}^*, \\mathbf{u}^*) = \\frac{\\partial f}{\\partial x_i}(\\mathbf{x}^*) + \\sum_{k=1}^{p} v_k^* \\frac{\\partial h_k}{\\partial x_i}(\\mathbf{x}^*) + \\sum_{j=1}^{m} u_j^* \\frac{\\partial g_j}{\\partial x_i}(\\mathbf{x}^*) = 0 \\quad \\text{for } i = 1, \\ldots, n\\) This condition ensures that at the optimum, the gradient of the objective function is a linear combination of the gradients of the active constraints. Geometrically, it means that the objective function‚Äôs gradient is ‚Äúaligned‚Äù with the combined gradients of the active constraints, preventing further improvement without violating constraints.\nFeasibility of all constraints: \\(h_k(\\mathbf{x}^*) = 0 \\quad \\text{for } k = 1, \\ldots, p\\) \\(g_j(\\mathbf{x}^*) \\le 0 \\quad \\text{for } j = 1, \\ldots, m\\) The optimal solution must satisfy all the original problem‚Äôs constraints.\nComplementary Slackness Conditions: \\(u_j^* g_j(\\mathbf{x}^*) = 0 \\quad \\text{for } j = 1, \\ldots, m\\) This is a crucial condition for inequality constraints. It implies that for each inequality constraint \\(g_j(\\mathbf{x}^*)\\):\n\nIf \\(g_j(\\mathbf{x}^*) &lt; 0\\) (the constraint is inactive, meaning the optimal solution is strictly within its allowable region), then its corresponding Lagrange multiplier \\(u_j^*\\) must be zero.\nIf \\(u_j^* &gt; 0\\), then the constraint \\(g_j(\\mathbf{x}^*)\\) must be active (i.e., \\(g_j(\\mathbf{x}^*) = 0\\)), meaning the optimal solution lies on the boundary defined by this constraint. This condition helps to determine which inequality constraints are active at the optimum.\n\nNon-negativity of Lagrange multipliers for inequality constraints: \\(u_j^* \\ge 0 \\quad \\text{for } j = 1, \\ldots, m\\) Lagrange multipliers for inequality constraints cannot be negative. This is consistent with the physical interpretation that relaxing a constraint (making its limit less restrictive) should not worsen the objective for a minimization problem. If \\(u_j &lt; 0\\), it means relaxing that constraint would make the objective worse, which is contradictory to its role.\n\nSolving these KKT conditions for \\(\\mathbf{x}^*\\), \\(\\mathbf{v}^*\\), and \\(\\mathbf{u}^*\\) gives us the candidate local optimum points. A superscript ‚Äú*‚Äù on a variable typically indicates its optimum value.\n\nPhysical Meaning of Lagrange Multipliers\nLagrange multipliers have an important physical meaning, particularly in engineering design: they represent the sensitivity of the optimal objective function value to a change in the constraint limit. This is also referred to as ‚Äúshadow prices‚Äù in economic contexts.\n\nFor an active inequality constraint \\(g_j(\\mathbf{x}^*) = 0\\) with \\(u_j^* &gt; 0\\), a positive \\(u_j^*\\) indicates that making the constraint slightly more restrictive (e.g., reducing the upper limit \\(b_j\\) in \\(g_j(\\mathbf{x}) \\le b_j\\)) would increase the minimum objective function value (for a minimization problem). Conversely, relaxing the constraint would decrease the objective. The magnitude of \\(u_j^*\\) quantifies this rate of change.\nFor an inactive inequality constraint \\(g_j(\\mathbf{x}^*) &lt; 0\\), its corresponding \\(u_j^*\\) is 0, meaning a small change in its limit has no effect on the optimum objective function value, because the design is not ‚Äúhitting‚Äù that constraint.\nFor equality constraints \\(h_k(\\mathbf{x}^*) = 0\\), \\(v_k^*\\) can be positive, negative, or zero. Its sign indicates whether relaxing or tightening the constraint would increase or decrease the objective.\n\nThis post-optimality or sensitivity analysis is extremely valuable for designers, as it tells them which constraints are critical and how much the design‚Äôs performance would change if those constraints were modified. In Excel Solver, Lagrange multiplier values are sometimes called Shadow Prices, and it‚Äôs important to note that the sign convention in Solver might be opposite to that used in academic texts, requiring a flip to match.\n\n\n\nSecond-Order Conditions (Brief Mention)\nThe KKT conditions are necessary conditions for a local minimum. This means that if a point is a local minimum, it must satisfy the KKT conditions. However, satisfying the KKT conditions does not guarantee that the point is a local minimum (it could be a local maximum or a saddle point). To confirm that a KKT point is indeed a local minimum, second-order sufficiency conditions are needed. These conditions involve the Hessian matrix of the Lagrangian function, \\(\\nabla^2 L(\\mathbf{x}^*, \\mathbf{v}^*, \\mathbf{u}^*)\\). For a point to be an isolated local minimum, this Hessian (or a related quadratic form) must be positive definite under certain conditions related to feasible directions. These second-order conditions are more advanced and are typically covered in graduate-level courses.\n\n\nSolved Example: Minimum Distance to Origin with a Linear Constraint\nLet‚Äôs apply the KKT conditions to a classic problem:\nProblem Statement: Minimize the square of the distance from the origin to a point \\((x_1, x_2)\\) in a 2D plane, subject to a linear inequality constraint and non-negativity of variables.\nFormulation: Minimize \\(f(x_1, x_2) = x_1^2 + x_2^2\\)\nSubject to: \\(g_1: x_1 + x_2 - 1 \\le 0\\) \\(g_2: -x_1 \\le 0 \\quad (\\text{i.e., } x_1 \\ge 0)\\) \\(g_3: -x_2 \\le 0 \\quad (\\text{i.e., } x_2 \\ge 0)\\)\n\nStep 1: Formulate the Lagrangian Function\n\\(L(x_1, x_2, u_1, u_2, u_3) = (x_1^2 + x_2^2) + u_1(x_1 + x_2 - 1) + u_2(-x_1) + u_3(-x_2)\\)\n\n\nStep 2: Write Down the KKT Necessary Conditions\n\nGradient conditions: \\(\\frac{\\partial L}{\\partial x_1} = 2x_1 + u_1 - u_2 = 0 \\quad \\text{(a)}\\) \\(\\frac{\\partial L}{\\partial x_2} = 2x_2 + u_1 - u_3 = 0 \\quad \\text{(b)}\\)\nFeasibility conditions: \\(x_1 + x_2 - 1 \\le 0 \\quad \\text{(c)}\\) \\(-x_1 \\le 0 \\quad \\Rightarrow x_1 \\ge 0 \\quad \\text{(d)}\\) \\(-x_2 \\le 0 \\quad \\Rightarrow x_2 \\ge 0 \\quad \\text{(e)}\\)\nComplementary slackness conditions: \\(u_1(x_1 + x_2 - 1) = 0 \\quad \\text{(f)}\\) \\(u_2(-x_1) = 0 \\quad \\text{(g)}\\) \\(u_3(-x_2) = 0 \\quad \\text{(h)}\\)\nNon-negativity of multipliers: \\(u_1 \\ge 0, u_2 \\ge 0, u_3 \\ge 0 \\quad \\text{(i)}\\)\n\n\n\nStep 3: Solve the KKT System by Considering Different Cases\nWe need to consider all possible combinations of active/inactive constraints. The complementary slackness conditions (f), (g), (h) are key here.\nCase 1: All inequality constraints are inactive. If \\(g_1 &lt; 0\\), \\(g_2 &lt; 0\\), \\(g_3 &lt; 0\\), then from complementary slackness (f, g, h), we must have \\(u_1 = 0, u_2 = 0, u_3 = 0\\). Substitute these into gradient conditions (a) and (b): \\(2x_1 = 0 \\Rightarrow x_1 = 0\\) \\(2x_2 = 0 \\Rightarrow x_2 = 0\\) So, the candidate point is \\((0,0)\\). Now, check feasibility condition (c) at \\((0,0)\\): \\(0 + 0 - 1 = -1 \\le 0\\). This is satisfied. However, we assumed \\(g_1 &lt; 0\\) (i.e., \\(x_1+x_2-1 &lt; 0\\)). At \\((0,0)\\), \\(x_1+x_2-1 = -1 &lt; 0\\), which is consistent. So, \\((x_1, x_2) = (0,0)\\) is a KKT point, with \\(u_1 = u_2 = u_3 = 0\\). The objective function value \\(f(0,0) = 0^2 + 0^2 = 0\\).\nCase 2: Constraint \\(g_1\\) is active, and \\(g_2, g_3\\) are inactive. If \\(g_1 = 0\\), then \\(x_1 + x_2 - 1 = 0 \\quad \\text{(j)}\\) If \\(g_2 &lt; 0 \\Rightarrow x_1 &gt; 0\\), then \\(u_2 = 0\\) (from (g)) If \\(g_3 &lt; 0 \\Rightarrow x_2 &gt; 0\\), then \\(u_3 = 0\\) (from (h))\nFrom (a) and (b) with \\(u_2=0, u_3=0\\): \\(2x_1 + u_1 = 0 \\Rightarrow u_1 = -2x_1\\) \\(2x_2 + u_1 = 0 \\Rightarrow u_1 = -2x_2\\) So, \\(-2x_1 = -2x_2 \\Rightarrow x_1 = x_2\\). Substitute \\(x_1 = x_2\\) into (j): \\(x_1 + x_1 - 1 = 0 \\Rightarrow 2x_1 = 1 \\Rightarrow x_1 = 0.5\\). So, \\(x_2 = 0.5\\). The candidate point is \\((0.5, 0.5)\\). Now, calculate \\(u_1\\): \\(u_1 = -2(0.5) = -1\\). However, KKT condition (i) requires \\(u_1 \\ge 0\\). Since \\(u_1 = -1\\), this case is not a valid KKT point. This means that the optimum cannot occur with \\(g_1\\) active and \\(x_1, x_2\\) strictly positive. This indicates that one or both of \\(x_1, x_2\\) must be zero at the optimum, making \\(g_2\\) or \\(g_3\\) active.\nCase 3: Constraints \\(g_1\\) and \\(g_2\\) are active, and \\(g_3\\) is inactive. If \\(g_1 = 0\\), then \\(x_1 + x_2 - 1 = 0 \\quad \\text{(j)}\\) If \\(g_2 = 0\\), then \\(-x_1 = 0 \\Rightarrow x_1 = 0 \\quad \\text{(k)}\\) If \\(g_3 &lt; 0 \\Rightarrow x_2 &gt; 0\\), then \\(u_3 = 0\\) (from (h))\nSubstitute \\(x_1 = 0\\) into (j): \\(0 + x_2 - 1 = 0 \\Rightarrow x_2 = 1\\). The candidate point is \\((0, 1)\\). Check condition \\(x_2 &gt; 0\\): \\(1 &gt; 0\\), so consistent. Now, find \\(u_1, u_2\\) (since \\(u_3=0\\)): From (a) with \\(x_1=0\\): \\(2(0) + u_1 - u_2 = 0 \\Rightarrow u_1 = u_2\\). From (b) with \\(x_2=1\\): \\(2(1) + u_1 - u_3 = 0 \\Rightarrow 2 + u_1 - 0 = 0 \\Rightarrow u_1 = -2\\). This requires \\(u_1 = -2\\). Since \\(u_1 \\ge 0\\) (condition (i)), this is not a valid KKT point.\nCase 4: Constraints \\(g_1\\) and \\(g_3\\) are active, and \\(g_2\\) is inactive. This case is symmetric to Case 3. It would lead to \\((1,0)\\) and \\(u_1 = -2\\), which is also not a valid KKT point.\nCase 5: Constraints \\(g_2\\) and \\(g_3\\) are active, and \\(g_1\\) is inactive. If \\(g_2 = 0 \\Rightarrow x_1 = 0 \\quad \\text{(k)}\\) If \\(g_3 = 0 \\Rightarrow x_2 = 0 \\quad \\text{(l)}\\) The candidate point is \\((0,0)\\). This is the same point as in Case 1. The difference is the active constraints. Here, \\(u_1=0\\) (because \\(g_1\\) is inactive). From (a) with \\(x_1=0, u_1=0\\): \\(0 + 0 - u_2 = 0 \\Rightarrow u_2 = 0\\). From (b) with \\(x_2=0, u_1=0\\): \\(0 + 0 - u_3 = 0 \\Rightarrow u_3 = 0\\). So, \\((0,0)\\) with \\(u_1 = 0, u_2 = 0, u_3 = 0\\) is a KKT point. \\(f(0,0)=0\\).\nCase 6: Constraints \\(g_1, g_2, g_3\\) are all active. \\(x_1 + x_2 - 1 = 0 \\quad \\text{(j)}\\) \\(x_1 = 0 \\quad \\text{(k)}\\) \\(x_2 = 0 \\quad \\text{(l)}\\) Substitute (k) and (l) into (j): \\(0 + 0 - 1 = 0 \\Rightarrow -1 = 0\\), which is a contradiction. This means these three constraints cannot be simultaneously active. So, no solution from this case.\nLet‚Äôs re-examine Case 2. The issue was \\(u_1 = -1\\). A correct KKT point from Case 2 would need \\(u_1 \\ge 0\\). So we need to consider combinations where \\(u_1\\) turns out positive.\nLet‚Äôs consider the scenario where \\(g_1\\) is active, and \\(u_1 &gt; 0\\). If \\(u_1 &gt; 0\\), then \\(x_1+x_2-1=0\\). From (a) and (b): \\(2x_1 + u_1 - u_2 = 0\\) \\(2x_2 + u_1 - u_3 = 0\\)\nSubcase 2.1: \\(g_1\\) active, \\(g_2\\) active, \\(g_3\\) inactive (\\(u_1 &gt; 0, u_2 &gt; 0, u_3 = 0\\)) \\(x_1 + x_2 - 1 = 0\\) \\(x_1 = 0 \\Rightarrow x_2 = 1\\) Point is \\((0,1)\\). From (b) with \\(x_2=1, u_3=0\\): \\(2(1) + u_1 = 0 \\Rightarrow u_1 = -2\\). Violates \\(u_1 \\ge 0\\). Invalid.\nSubcase 2.2: \\(g_1\\) active, \\(g_3\\) active, \\(g_2\\) inactive (\\(u_1 &gt; 0, u_3 &gt; 0, u_2 = 0\\)) \\(x_1 + x_2 - 1 = 0\\) \\(x_2 = 0 \\Rightarrow x_1 = 1\\) Point is \\((1,0)\\). From (a) with \\(x_1=1, u_2=0\\): \\(2(1) + u_1 = 0 \\Rightarrow u_1 = -2\\). Violates \\(u_1 \\ge 0\\). Invalid.\nSubcase 2.3: \\(g_1\\) active, \\(g_2\\) inactive, \\(g_3\\) inactive (\\(u_1 &gt; 0, u_2 = 0, u_3 = 0\\)) \\(x_1 + x_2 - 1 = 0\\) From (a): \\(2x_1 + u_1 = 0 \\Rightarrow u_1 = -2x_1\\) From (b): \\(2x_2 + u_1 = 0 \\Rightarrow u_1 = -2x_2\\) So \\(x_1 = x_2\\). Substitute into \\(x_1 + x_2 - 1 = 0 \\Rightarrow 2x_1 = 1 \\Rightarrow x_1 = 0.5\\). Thus \\(x_2 = 0.5\\). Point is \\((0.5, 0.5)\\). \\(u_1 = -2(0.5) = -1\\). Violates \\(u_1 \\ge 0\\). Invalid.\nIt seems I made a mistake by prematurely dismissing \\((0,0)\\) as a single valid point. Let‚Äôs look closer at the problem.\nThe objective is \\(f(x_1, x_2) = x_1^2 + x_2^2\\), which is minimized at \\((0,0)\\). The feasible region is the area \\(x_1+x_2 \\le 1\\) in the first quadrant (\\(x_1 \\ge 0, x_2 \\ge 0\\)). The point \\((0,0)\\) is in the feasible region, and \\(f(0,0)=0\\).\nLet‚Äôs check the point \\((0.5, 0.5)\\) again carefully. At \\((0.5, 0.5)\\), \\(g_1 = 0.5 + 0.5 - 1 = 0\\). So \\(g_1\\) is active. \\(g_2 = -0.5 &lt; 0\\). So \\(g_2\\) is inactive, meaning \\(u_2 = 0\\). \\(g_3 = -0.5 &lt; 0\\). So \\(g_3\\) is inactive, meaning \\(u_3 = 0\\). From KKT (a): \\(2x_1 + u_1 - u_2 = 0 \\Rightarrow 2(0.5) + u_1 - 0 = 0 \\Rightarrow 1 + u_1 = 0 \\Rightarrow u_1 = -1\\). Since \\(u_1\\) must be \\(\\ge 0\\), this point \\((0.5, 0.5)\\) is not a valid KKT point. This means that if \\(x_1, x_2 &gt; 0\\), the optimum cannot be on the line \\(x_1+x_2=1\\).\nThe only remaining valid KKT point is \\((0,0)\\) from Case 1 (or Case 5 which leads to the same point and multipliers). At \\((0,0)\\): \\(g_1 = 0+0-1 = -1 \\le 0\\) (inactive, so \\(u_1=0\\)) \\(g_2 = -0 = 0 \\le 0\\) (active, so \\(u_2 \\ge 0\\)) \\(g_3 = -0 = 0 \\le 0\\) (active, so \\(u_3 \\ge 0\\))\nWith \\(u_1=0\\): From (a): \\(2x_1 + 0 - u_2 = 0 \\Rightarrow 2x_1 = u_2\\). At \\((0,0)\\), \\(u_2 = 0\\). From (b): \\(2x_2 + 0 - u_3 = 0 \\Rightarrow 2x_2 = u_3\\). At \\((0,0)\\), \\(u_3 = 0\\). So, \\(u_1=0, u_2=0, u_3=0\\). All non-negativity conditions for multipliers are met. Thus, \\(\\mathbf{x}^* = (0,0)\\) is a valid KKT point, with \\(f(\\mathbf{x}^*) = 0\\).\nThis result is consistent: the unconstrained minimum \\((0,0)\\) is within the feasible region (\\(0+0-1 \\le 0\\), \\(0 \\ge 0\\), \\(0 \\ge 0\\)), so the constraints do not restrict the optimal solution, and thus the Lagrange multipliers are all zero. The constraints are ‚Äúslack‚Äù or ‚Äúnon-binding‚Äù at the true optimum for this specific problem.\n\n\nConclusion for the Example\nThe only point that satisfies all KKT conditions is \\(\\mathbf{x}^* = (0,0)\\), with the objective function value \\(f(\\mathbf{x}^*) = 0\\). The Lagrange multipliers are \\(u_1^* = 0\\), \\(u_2^* = 0\\), and \\(u_3^* = 0\\). This indicates that none of the constraints are actively ‚Äúpushing‚Äù the solution away from the origin; the unconstrained minimum is already feasible.\n\nThis concludes our introduction to Lagrangian Methods and the KKT conditions. These analytical tools are foundational for understanding and developing more advanced numerical methods for constrained optimization, which we will explore in subsequent lectures. While solving KKT conditions algebraically can be complex for highly nonlinear problems, the principles provide profound insights into the nature of optimal design solutions.",
    "crumbs": [
      "üìñ Lectures",
      "Lecture 5: Lagrangian Methods"
    ]
  },
  {
    "objectID": "documents/09-Lecture_07.html",
    "href": "documents/09-Lecture_07.html",
    "title": "Lecture 7: Applications of Design Optimization",
    "section": "",
    "text": "Lecture 7: Applications of Design Optimization\nWelcome to our final scheduled lecture, where we bring together all the theoretical and methodological concepts we‚Äôve covered and demonstrate their profound impact on real-world engineering. Today, we will explore various applications of design optimization across different engineering disciplines, focusing particularly on mechanical engineering systems. The goal is to illustrate how the systematic approach of optimum design leads to superior, more efficient, and cost-effective solutions in practice.\nDesign optimization is not just an academic exercise; it‚Äôs an indispensable tool in modern engineering. From the smallest component to the largest structural system, engineers are constantly challenged to improve performance, reduce costs, enhance reliability, and ensure safety. Design optimization provides the structured framework to meet these challenges.\n\n1. Structural Engineering Applications\nStructural engineering is a prime field for design optimization, where objectives often include minimizing weight, material cost, or manufacturing effort, subject to strict constraints on stress, deflection, buckling, and fatigue.\n\nExample: Minimum-Weight Column/Beam Design\nRecall our earlier example of a tubular column or a rectangular beam. The objective is typically to minimize the material volume or mass, which directly relates to cost. The design variables might be the cross-sectional dimensions (e.g., mean radius and wall thickness for a column, or width and height for a beam).\nThe constraints are crucial: * Strength Constraints: The stresses developed under expected loads (axial, bending, shear) must not exceed the material‚Äôs allowable stress or yield strength. These often involve nonlinear relationships with the design variables. * Stability Constraints: For columns, buckling must be prevented. The critical buckling load must be greater than the applied load, which is also a nonlinear function of geometry and material properties. * Deflection Constraints: The deformation of the structure under load must be within acceptable limits (e.g., to prevent cracking of finishes, or to ensure functionality). * Geometric/Manufacturing Constraints: Practical limits on dimensions, such as minimum wall thickness or maximum allowable aspect ratios for manufacturability or to prevent local buckling.\nNumerical methods like Sequential Linear Programming (SLP) or Sequential Quadratic Programming (SQP) are essential here, as the objective and constraint functions are typically nonlinear. For example, optimizing a two-member frame involves dealing with several design variables and constraints related to member stresses, fundamental frequency, and geometry. The iterative numerical process helps navigate this complex design space to find the optimal configuration.\n\n\nExample: Optimal Design of Steel Structures using Standard Sections\nIn practice, engineers often select components from a predefined list of standard sections (e.g., I-beams, channels, angles). This introduces discrete design variables, where the choice is not a continuous range but a selection from a catalog. Problems of this nature fall under Mixed Variable Optimization (MV-OPT).\nHere, the optimization might involve: * Objective: Minimize the weight of the steel structure. * Design Variables: The standard section designation (e.g., W10x33, W12x26) for each structural member. * Constraints: All the usual structural constraints (stress, deflection, buckling) as per design codes (e.g., AISC specifications), but now the properties (area, moment of inertia, section modulus) are looked up from the chosen discrete section.\nThese problems are significantly more challenging than continuous optimization and often require specialized methods like branch-and-bound, simulated annealing, or genetic algorithms, which can effectively search discrete solution spaces.\n\n\n\n2. Mechanical System Design Applications\nOptimization is integral to designing mechanical components and systems for performance, durability, and cost-effectiveness.\n\nExample: Optimal Design of a Helical Spring\nConsider the design of a compression helical spring for a given application. This is a classic example of a nonlinear programming problem.\n\nObjective: Often to minimize the mass or volume of the spring, which relates to material usage and space.\nDesign Variables:\n\n\\(d\\): wire diameter\n\\(D\\): mean coil diameter\n\\(N\\): number of active coils\n\nConstraints:\n\nShear Stress: The shear stress in the spring wire must not exceed the allowable shear stress for the material. This is a complex nonlinear function of \\(d, D\\), and the applied force.\nDeflection: The spring must be able to achieve a specified deflection without going solid or exceeding its elastic limit.\nSurge Frequency: The natural frequency of the spring must be sufficiently higher than the excitation frequency to avoid resonance.\nGeometric Constraints: Limits on the outer diameter (to fit in space), free length, and sometimes a minimum number of coils for stability.\n\n\nSolving this problem analytically for all KKT conditions would be arduous due to the nonlinearities. Numerical methods, often implemented in tools like Excel Solver (using the GRG Nonlinear method), are employed to find the optimal combination of \\(d, D,\\) and \\(N\\) that satisfies all requirements while minimizing mass.\n\n\n\n3. Manufacturing and Process Optimization\nOptimization extends beyond the physical design of components to the planning and sequencing of manufacturing processes.\n\nExample: Bolt Insertion Sequence or Welding Sequence\nImagine a robotic arm tasked with inserting bolts into a predrilled metal plate or welding various points on an automotive subassembly. The objective is to minimize the total travel distance or time of the robotic arm to complete all operations.\n\nObjective: Minimize total travel distance/time.\nDesign Variables: The sequence in which the operations (bolt insertions, welds) are performed. If there are \\(N\\) locations, there are \\(N!\\) possible sequences.\nConstraints: Each location must be visited exactly once.\n\nThis is a variant of the famous Traveling Salesman Problem (TSP), which is a discrete optimization problem. Since the number of permutations grows factorially, full enumeration quickly becomes impossible for even a moderate number of locations (e.g., 10! = 3,628,800, 16! \\(\\approx 2 \\times 10^{13}\\)). Nature-inspired search methods, particularly Genetic Algorithms (GAs), are well-suited for such problems. GAs employ principles of natural selection and evolution to efficiently search large, discrete solution spaces, finding near-optimal sequences in a fraction of the time required by exhaustive search.\n\n\n\n4. Advanced Applications and Cross-Disciplinary Areas\nDesign optimization is also applied in more complex and interdisciplinary fields:\n\nOptimal Control: While distinct from optimum design, many optimal control problems can be rephrased as optimization problems. For instance, determining the optimal control forces to bring a dynamic system to rest in minimum time or with minimum energy consumption can be formulated as a nonlinear programming problem. Here, the ‚Äúdesign variables‚Äù become parameters defining the control strategy over time.\nRobust Design (Taguchi Methods): This approach focuses on designing systems that are insensitive to variations in manufacturing, operating conditions, or environmental factors. The objective often involves minimizing a ‚Äúloss function‚Äù that accounts for both the mean performance and its variability.\nReliability-Based Design Optimization (RBDO): Instead of deterministic constraints (e.g., stress \\(\\le\\) allowable stress), RBDO incorporates the probabilistic nature of loads, material properties, and manufacturing tolerances. The constraints are formulated to ensure a certain ‚Äúprobability of failure‚Äù is below a target value (e.g., 99.9% reliability).\n\n\n\n5. Engineering Economic Analysis\nIn specific scenarios, formal mathematical optimization techniques, such as calculus, are employed to find minimum or maximum points for costs or profits:\n\nCost Minimization: Total Cost (C) or Net Annual Cost (NAC) can be expressed as a function of a variable (like production volume, \\(Q\\), or amount of chemical purchased, \\(X\\)). To find the minimum total cost, the first derivative of the cost function is set to zero (\\(\\text{dC/dQ}=0\\)). This process confirms the production rate at which total cost is minimized.\nProfit Maximization: Similarly, when Total Profit (\\(\\text{TP}\\)) is a function of production volume (\\(x\\)), the production level that maximizes profit is found by setting the first derivative (\\(\\text{dTP/dx}\\)) equal to zero.\nEconomic Life: The determination of an asset‚Äôs minimum cost life relies on finding the number of years that minimizes the EUAC. This minimum point represents the optimal balance between the high annual capital cost of short ownership and the increasing operating and maintenance (O&M) costs associated with extended ownership. Spreadsheets are widely used to compute the optimal economic life by finding the EUAC minimum.\n\n\nüîó Visit the Engineering Economics Analysis Course ‚Äî Learn complementary Engineering Economics Analysis Techniques\n\n\n\nConclusion\nAs you can see, design optimization is a versatile and powerful discipline. From optimizing the dimensions of a structural element for minimal weight to planning the most efficient sequence of operations on a factory floor, the principles and methods we‚Äôve studied are directly applicable to a vast array of engineering challenges. The ability to mathematically formulate a problem, identify the objective, define constraints, and apply appropriate numerical methods is a fundamental skill for any engineer seeking to create truly optimal designs in today‚Äôs competitive and resource-conscious world.\nThis concludes our lectures on Design Optimization. I hope you‚Äôve gained a comprehensive understanding of how to systematically approach engineering design problems to find the best possible solutions. Good luck with your future design endeavors!",
    "crumbs": [
      "üìñ Lectures",
      "Lecture 7: Applications"
    ]
  },
  {
    "objectID": "documents/11-questions.html",
    "href": "documents/11-questions.html",
    "title": " Design Optimization",
    "section": "",
    "text": "A list of questions, identified by their numbers from the J.S. Arora, Introduction to optimum Design, Academic Press, 2016. These questions are selected to cover the concepts discussed in each lecture.\n\n\n\n\n\n\n\nLecture #\nQuestions #\n\n\n\n\n02\n2.6, 2.7, 2.8, 2.9, 2.10, 2.11, 2.12, 2.13, 2.14, 2.18, 2.20, 2.21\n\n\n03\n3.23, 3.24, 3.25, 3.26, 3.34, 3.36, 3.37, 3.38, 3.39, 3.40, 3.41, 3.42, 3.43, 3.44, 3.45, 3.46, 3.50, 3.51, 3.52, 3.53, 3.54\n\n\n04\n8.1, 8.20, 8.55, 8.61, 8.62, 8.63, 8.64, 8.81, 8.83, 8.86, 8.110, 8.111, 8.116, 8.117, 8.118, 8.119, 8.120, 8.132, 8.138, 8.159\n\n\n05\n4.1, 4.2, 4.22, 4.26, 4.27, 4.28, 4.29, 4.30, 4.31, 4.32, 4.43, 4.44, 4.45, 4.46, 4.47, 5.43, 5.49\n\n\n06\n12.1, 12.2, 12.3, 12.4, 12.16, 12.17, 12.18, 12.29, 12.30, 13.1, 13.2, 13.3, 13.4, 13.5, 13.6, 13.7, 13.8, 13.9\n\n\n07\n14.1, 14.9, 14.13, 14.17, 14.23, 15.1, 15.3, 16.1, 16.11, 17.1, 17.3, 17.15, 18.5, 18.7, 19.1, 19.3, 19.4, 19.8, 19.9, 19.10",
    "crumbs": [
      "‚ùì Quiz Questions"
    ]
  }
]